{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clasificación de tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning con BertForSequenceClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autor: Alberto Ramos Sánchez\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contenido\n",
    "* [Dataset: TASS](#Dataset:-TASS)\n",
    "* [Preparar tweets](#Preparar-tweets)\n",
    "* [Crear dataset de entrenamiento](#Crear-dataset-de-entrenamiento)\n",
    "    * [Tokenizar contenido](#Tokenizar-contenido)\n",
    "    * [Sets de entrenamiento, validación y test](#Sets-de-entrenamiento,-validación-y-test)\n",
    "    * [Crear dataset](#Crear-dataset)\n",
    "* [Modelo de clasificación: *BertForSequenceClassification*](#Modelo-de-clasificación:-*BertForSequenceClassification*)\n",
    "* [Entrenamiento](#Entrenamiento)\n",
    "    * [Evaluación](#Evaluación)\n",
    "* [Resultados](#Resultados)\n",
    "    * [Con 3 clases](#Con-3-clases)\n",
    "        * [Entrenamiento](#Entrenamiento)\n",
    "        * [Validación](#Validación)\n",
    "        * [Test](#Test)\n",
    "    * [Con 6 clases](#Con-6-clases)\n",
    "        * [Entrenamiento](#Entrenamiento)\n",
    "        * [Validation](#Validación)\n",
    "        * [Test](#Test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import datetime\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import SnowballStemmer\n",
    "stemmer = SnowballStemmer('spanish')\n",
    "\n",
    "import re\n",
    "import string\n",
    "\n",
    "from pathlib import Path\n",
    "import gzip\n",
    "\n",
    "import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import Dataset, IterableDataset, DataLoader\n",
    "\n",
    "from transformers import BertTokenizer, BertModel, AdamW\n",
    "\n",
    "from pytorchtools.pytorchtools import EarlyStopping\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)\n",
    "\n",
    "# seed\n",
    "import random\n",
    "\n",
    "seed = 42\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este *notebook* se ha entrenado el modelo *Bert* para realizar *sentiment analysis* sobre el conjunto de tweets en español de TASS. El entrenamiento se ha llevado a cabo haciendo *finetuning* sobre la versión para español de *Bert* [*BETO*](https://github.com/dccuchile/beto)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset: TASS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se ha trabajado con los *datasets* de tweets de [*TASS SEPLN*](http://tass.sepln.org/tass_data/download.php) desde el año 2012 al 2019. \n",
    "Todos los ficheros *xml* se han unido en un único dataset contenido en los ficheros *tweets.csv.gz*, *topics.csv.gz* y *polarities.csv.gz*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = Path(\"./TASS/conversion_result/dataset31k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets = pd.read_csv(dataset_path / Path(\"tweets.csv.gz\"), compression='gzip', header=0, sep=';', quotechar='\"')\n",
    "df_topics = pd.read_csv(dataset_path / Path(\"topics.csv.gz\"), compression='gzip', header=0, sep=';', quotechar='\"')\n",
    "df_polarities = pd.read_csv(dataset_path / Path(\"polarities.csv.gz\"), compression='gzip', header=0, sep=';', quotechar='\"')\n",
    "\n",
    "df_topics = df_topics.rename(columns={'tweetid_fk': 'tweetid'})\n",
    "df_polarities = df_polarities.rename(columns={'tweetid_fk': 'tweetid'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En total, en el dataset hay aproximadamente 31 mil tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Número total de tweets = 31375'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"Número total de tweets = {len(df_tweets)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y están etiquetados en 6 categorías:\n",
    "\n",
    "* P+ : Positivo fuerte (strong negative)\n",
    "* P : Positivo\n",
    "* NONE : Sin sentimiento (no sentiment tag)\n",
    "* NEU : Neutro\n",
    "* N : Negativo\n",
    "* N+ : Negativo fuerte (strong negative)\n",
    "\n",
    "En la siguiente tabla se muestra la cantidad de tweets por categoría:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Número de tweets</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>value</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>N</th>\n",
       "      <td>6219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N+</th>\n",
       "      <td>976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NEU</th>\n",
       "      <td>2755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NONE</th>\n",
       "      <td>5597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P</th>\n",
       "      <td>5442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P+</th>\n",
       "      <td>2793</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Número de tweets\n",
       "value                  \n",
       "N                  6219\n",
       "N+                  976\n",
       "NEU                2755\n",
       "NONE               5597\n",
       "P                  5442\n",
       "P+                 2793"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_polarities[['value', 'tweetid']].drop_duplicates(subset='tweetid', keep=\"first\").groupby(\"value\").count().rename(columns={\"value\": \"Categoría\", \"tweetid\": \"Número de tweets\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparar tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este apartado preprocesamos el texto de los tweets para entrenar el modelo.\n",
    "\n",
    "En la función *clean_tweet* limpiamos el texto quedándonos solamente con los caractéres alfanuméricos. Eliminamos los nombres de usuario, url, vocales seguidas repetidas más de dos veces, tabulaciones, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweet(text):\n",
    "    res_txt = re.sub(\"@\\w+\", \"\", text) # drop username\n",
    "    res_txt = re.sub(\"https?://[A-Za-z0-9\\./]+\", \"\", res_txt) # drop url\n",
    "    \n",
    "    for p in string.punctuation:\n",
    "        res_txt = res_txt.replace(p, \" \")\n",
    "    #res_txt = \" \".join([c for c in res_txt if c not in string.punctuation])\n",
    "    \n",
    "    # eliminar palabras con más de 2 vocales seguidas \"largooooo -> largoo\"\n",
    "    res_txt = re.sub(\"([A-Za-z])\\\\1{2,}\", \"\\\\1\\\\1\", res_txt)\n",
    "    \n",
    "    # eliminar espacios y tabulaciones repetidas\n",
    "    res_txt = re.sub(\"[ \\t]+\", \" \", res_txt.strip())\n",
    "    \n",
    "    # mantener solamente caracteres alfanuméricos\n",
    "    res_txt = re.sub(r'[^a-zñÑA-Z0-9áéíóúÁÉÍÓÚ ]', '', res_txt)\n",
    "    return res_txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En *prepare_tweet* eliminamos las *stopwords*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_tweet(text):\n",
    "    stop_words = nltk.corpus.stopwords.words('spanish')\n",
    "    custom_stop_words = [\"d\", \"q\"]\n",
    "    \n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [w for w in tokens if w not in stop_words]\n",
    "    tokens = [w for w in tokens if w not in custom_stop_words]\n",
    "    tokens = [w.lower() for w in tokens]\n",
    "    \n",
    "    # stemming\n",
    "    #tokens = [stemmer.stem(w) for w in tokens]\n",
    "    \n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplicamos el preprocesado, obteniendo el siguiente resultado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original :\n",
      " ;-)) RT @doloresvela: El #iPhone 5 podría ser presentado en marzo http://t.co/2kjKTjfF\n",
      "Resultado :\n",
      " rt el iphone 5 podría ser presentado marzo\n"
     ]
    }
   ],
   "source": [
    "print(f\"Original :\\n {df_tweets['content'][8235]}\")\n",
    "\n",
    "content = df_tweets['content']\n",
    "\n",
    "content = content.apply(lambda tweet: prepare_tweet(clean_tweet(str(tweet))))\n",
    "\n",
    "df_tweets['content'] = content\n",
    "\n",
    "print(f\"Resultado :\\n {df_tweets['content'][8235]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crear dataset de entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este apartado se crea el dataset de entrenamiento.\n",
    "\n",
    "Unimos los dataframes en uno solo, que contiene el id del tweet, el contenido del tweet y la categoría."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 23782 entries, 0 to 23781\n",
      "Data columns (total 3 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   tweetid  23782 non-null  int64 \n",
      " 1   content  23782 non-null  object\n",
      " 2   value    23782 non-null  object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 557.5+ KB\n"
     ]
    }
   ],
   "source": [
    "data_tweets = df_tweets[['tweetid', 'content']]\n",
    "data_sentim = df_polarities[['tweetid', 'value']]\n",
    "\n",
    "data_tweets = data_tweets.merge(data_sentim, on=\"tweetid\").drop_duplicates(subset='tweetid', keep=\"first\").reset_index(drop=True)\n",
    "data_tweets.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se cambia la etiqueta por un valor numérico que indica la clase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Número de tweets</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>value</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2793</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Número de tweets\n",
       "value                  \n",
       "0                  6219\n",
       "1                   976\n",
       "2                  5597\n",
       "3                  2755\n",
       "4                  5442\n",
       "5                  2793"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label2id = {\"N\": 0,\n",
    "            \"N+\": 1,\n",
    "            \"NONE\": 2,\n",
    "            \"NEU\": 3,\n",
    "            \"P\": 4,\n",
    "            \"P+\": 5}\n",
    "\n",
    "NUMBER_CLASSES = 6\n",
    "\n",
    "data_tweets[\"value\"].replace(label2id, inplace=True)\n",
    "\n",
    "data_tweets[['value', 'tweetid']].drop_duplicates(subset='tweetid', keep=\"first\").groupby(\"value\").count().rename(columns={\"value\": \"Categoría\", \"tweetid\": \"Número de tweets\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con la variable *apply_balance* controlamos si queremos aplicar o no balanceo de datos.\n",
    "\n",
    "En el caso de utilizar 6 clases, no aplicamos balanceo de datos, pues perderíamos la mayoría de los tweets debido a que la clase N+ tiene muchos menos tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_balance = False\n",
    "\n",
    "if apply_balance:\n",
    "    g = data_tweets.groupby('value')\n",
    "    data_tweets = g.apply(lambda x: x.sample(g.size().min()).reset_index(drop=True))\n",
    "    data_tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizar contenido"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se tokeniza el texto, convirtiendo las palabras a números enteros, y añadiendo un token de clasificación *[CLS]* al inicio de la frase. Para esto se utiliza la clase *BertTokenizer* con los pesos de *BETO*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_MODEL = 'dccuchile/bert-base-spanish-wwm-uncased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(BASE_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Buscamos el tweet de mayor longitud y asignamos un valor de longitud mayor, para que la longitud de las frases tokenizadas contenga todos los tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = data_tweets['content']\n",
    "tok = tweets.apply(lambda tuit: tokenizer.encode(tuit, add_special_tokens=True))\n",
    "tok.apply(lambda x: len(x)).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_token_len = 55\n",
    "\n",
    "def tokenizar(df):\n",
    "    # tokeniza texto y genera máscara de atención\n",
    "    tweets = df['content'].tolist()\n",
    "    tokenize_result = tokenizer(tweets,\n",
    "                                add_special_tokens=True, \n",
    "                                max_length=max_token_len,\n",
    "                                return_attention_mask=True,\n",
    "                                padding='max_length')\n",
    "    \n",
    "    data_tensor = torch.LongTensor(tokenize_result.input_ids)\n",
    "    mask_tensor = torch.LongTensor(tokenize_result.attention_mask)\n",
    "    \n",
    "    # crea etiquetas\n",
    "    polarity_id = df['value'].tolist()\n",
    "    \n",
    "    target_tensor = torch.tensor(polarity_id)\n",
    "    \n",
    "    return data_tensor, target_tensor, mask_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Además de tokenizar el texto, se crea un vector adicional llamado *attention mask*. Esta máscara indica al *transformer* donde está realmente el contenido de la frase, para así evitar aplicar mecanismos de atención sobre los tokens de *padding*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([    4,  1035, 10406, 30956,  1836,  1948,  2803,  2403,  1361, 20922,\n",
       "          1200,  1544, 20578, 30958,  1002, 28253,  1250,  7684,     5,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1]),\n",
       " tensor(4),\n",
       " tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data, target, mask = tokenizar(data_tweets)\n",
    "\n",
    "data[0], target[0], mask[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([    4,  1733,  1784, 30962, 11441,  2566, 30957,  2030, 15491,  2849,\n",
       "          1825, 17810, 30958, 11925,  1943, 14352,  1137,  5489,  4878, 30955,\n",
       "             5,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1]),\n",
       " ['[CLS]',\n",
       "  'nom',\n",
       "  '##eo',\n",
       "  '##l',\n",
       "  '##vido',\n",
       "  'aprob',\n",
       "  '##o',\n",
       "  'ley',\n",
       "  'aborto',\n",
       "  'libre',\n",
       "  'todas',\n",
       "  'ministra',\n",
       "  '##s',\n",
       "  'salta',\n",
       "  '##ban',\n",
       "  'alegr',\n",
       "  '##ia',\n",
       "  'congreso',\n",
       "  'llor',\n",
       "  '##e',\n",
       "  '[SEP]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]'])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[2], tokenizer.convert_ids_to_tokens(data[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sets de entrenamiento, validación y test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dividimos el *dataset* en entrenamiento, validación y test. Con la opción *stratify* aseguramos que se mantenga la misma proporción de clases para cada conjunto que el *dataset* original. Para asegurarlo, es posible que se pierdan ciertos tweets. Esta operación no balancea el dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 0.90\n",
    "val_size = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([21403, 55]), torch.Size([1189, 55]), torch.Size([1190, 55]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train, data_val, target_train, target_val, mask_train, mask_val = train_test_split(data, target, mask, test_size=1-train_size, stratify=target)\n",
    "\n",
    "data_val, data_test, target_val, target_test, mask_val, mask_test = train_test_split(data_val, target_val, mask_val, test_size=1-(val_size/(1-train_size)), stratify=target_val)\n",
    "\n",
    "data_train.shape, data_val.shape, data_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crear dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La clase *TweetDataset* permite iterar sobre el *dataset* devolviendo los *tokens*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, data, target, mask):\n",
    "        self.data = data\n",
    "        self.target = target\n",
    "        self.mask = mask\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.target[idx], self.mask[idx]\n",
    "\n",
    "train_dataset = TweetDataset(data_train, target_train, mask_train)\n",
    "val_dataset = TweetDataset(data_val, target_val, mask_val)\n",
    "test_dataset = TweetDataset(data_test, target_test, mask_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo de clasificación: *BertForSequenceClassification*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso utilizaremos de la librería *transformers* de [*HuggingFace*](https://huggingface.co/transformers/) la clase *BertForSequenceClassification*, que viene preparada para reutilizar *Bert* para tareas de clasificación de texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "MODEL = BertForSequenceClassification.from_pretrained(BASE_MODEL, num_labels=NUMBER_CLASSES).to(device)\n",
    "lr = 1e-6\n",
    "OPTIMIZER = AdamW(MODEL.parameters(), lr=lr)\n",
    "CRITERION = nn.CrossEntropyLoss()\n",
    "ITERATIONS = 100\n",
    "PATIENCE = 20\n",
    "\n",
    "batchsize = 8\n",
    "\n",
    "TRAIN_LOADER = DataLoader(train_dataset, batch_size=batchsize, shuffle=True)\n",
    "VAL_LOADER = DataLoader(val_dataset, batch_size=batchsize, shuffle=True)\n",
    "TEST_LOADER = DataLoader(test_dataset, batch_size=batchsize, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Según el ***warning*** anterior, se cargó todo el modelo excepto el clasificador, que es lo que vamos a entrenar desde cero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las dos siguientes funciones definen el entrenamiento y validación de la red. Mediante el flag *stop_train* interrumpiendo el *kernel* con el botón de *stop* puede parase el entrenamiento en cualquier momento.\n",
    "\n",
    "Para evitar sobrecargar la memoria de la GPU, las variables de cargan a CUDA por *batches* únicamente en el momento de utilizarlas.\n",
    "\n",
    "El modelo final se almacena en el directorio elegido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "global stop_train\n",
    "stop_train = False\n",
    "\n",
    "def evaluate(model, loss_function, val_loader):\n",
    "    \n",
    "    total_acc = 0\n",
    "    total_precision = 0\n",
    "    total_recall = 0\n",
    "    \n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch_step, (data, target, mask) in enumerate(val_loader):\n",
    "        try:\n",
    "            cuda_data = data.to(device)\n",
    "            cuda_target = target.to(device)\n",
    "            cuda_mask = mask.to(device)\n",
    "\n",
    "            output = model(cuda_data, cuda_mask).logits\n",
    "\n",
    "            loss = loss_function(output, cuda_target)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            prediction = output.argmax(dim=1)\n",
    "\n",
    "            acc = accuracy_score(prediction.cpu(), cuda_target.cpu())#(prediction == cuda_target).cpu().numpy().mean()\n",
    "            prec = precision_score(prediction.cpu(), cuda_target.cpu(), average=\"macro\", zero_division=0.0)\n",
    "            rec = recall_score(prediction.cpu(), cuda_target.cpu(), average=\"macro\", zero_division=0.0)\n",
    "\n",
    "            total_acc += acc\n",
    "            total_precision += prec\n",
    "            total_recall += rec\n",
    "        except KeyboardInterrupt:\n",
    "            try: del cuda_data\n",
    "            except: pass\n",
    "            \n",
    "            try: del cuda_target\n",
    "            except: pass\n",
    "            \n",
    "            try: del cuda_mask\n",
    "            except: pass\n",
    "            \n",
    "            stop_train = True\n",
    "            print(\"*** train-data variables removed from cuda memory ***\")\n",
    "            break\n",
    "        \n",
    "        # force to free cuda memory\n",
    "        del cuda_data\n",
    "        del cuda_target\n",
    "        del cuda_mask\n",
    "    T = len(val_loader)\n",
    "    return map(lambda r: r/T, [total_loss, total_acc, total_precision, total_recall])\n",
    "\n",
    "\n",
    "def train(folder, model, loss_function, optim, nepochs, patience, train_loader, val_loader):\n",
    "    model.train()\n",
    "    \n",
    "    ts = datetime.datetime.now()\n",
    "    best_model_path = Path(f\"./temp/best_model_{ts.day}{ts.month}{ts.year}{ts.hour}{ts.minute}{ts.second}\")\n",
    "    best_val_loss = np.inf\n",
    "    early_counter = 0\n",
    "    \n",
    "    \n",
    "    for epoch in range(1, nepochs+1):\n",
    "        \n",
    "        total_loss = 0\n",
    "        total_acc = 0\n",
    "        \n",
    "        progress_bar = tqdm.tqdm(train_loader, desc='Bar desc', leave=True)\n",
    "        \n",
    "        for batch_step, (data, target, mask) in enumerate(progress_bar, 1):\n",
    "            try:\n",
    "                data_cuda = data.to(device)\n",
    "                target_cuda = target.to(device)\n",
    "                mask_cuda = mask.to(device)\n",
    "\n",
    "                optim.zero_grad()\n",
    "\n",
    "                out = MODEL(data_cuda, mask_cuda).logits\n",
    "                loss = loss_function(out, target_cuda)\n",
    "\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                acc = accuracy_score(out.detach().cpu().argmax(dim=1), target)\n",
    "                total_acc += acc\n",
    "\n",
    "                loss.backward()\n",
    "\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "                optim.step()\n",
    "\n",
    "                progress_bar.set_description(f\"{batch_step = } / {len(train_loader)} / {epoch = } | loss = {total_loss / batch_step : 0.4f} acc = {total_acc / batch_step  : 0.4f}\")\n",
    "\n",
    "                del data_cuda\n",
    "                del target_cuda\n",
    "                del mask_cuda\n",
    "            except KeyboardInterrupt:\n",
    "                try: del cuda_data\n",
    "                except: pass\n",
    "                \n",
    "                try: del cuda_target\n",
    "                except: pass\n",
    "                \n",
    "                try: del cuda_mask\n",
    "                except: pass\n",
    "                \n",
    "                print(\"*** train-data variables removed from cuda memory ***\")\n",
    "                global stop_train\n",
    "                stop_train = True\n",
    "                break\n",
    "        \n",
    "        if stop_train: # stop from train\n",
    "            print(\"*** stoping training ***\")\n",
    "            break\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss, val_acc, val_prec, val_rec = evaluate(model, loss_function, val_loader)\n",
    "        model.train()\n",
    "        \n",
    "        state_ss = f\"Epoch : {epoch} / {nepochs} | loss = {total_loss / len(train_loader) : 0.4f} acc = {total_acc / len(train_loader) : 0.4f}\"\n",
    "        state_ss += f\" | {val_loss = : 0.4f} {val_acc = : 0.4f} {val_prec = : 0.4f} {val_rec = : 0.4f}\"\n",
    "        \n",
    "        print(state_ss)\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            print(f\"Validation loss decreased ({best_val_loss:0.4f} --> {val_loss:0.4f}).  Saving model ...\")\n",
    "            model.save_pretrained(best_model_path)\n",
    "            \n",
    "            best_val_loss = val_loss\n",
    "            early_counter = 0\n",
    "        else:\n",
    "            if early_counter < patience:\n",
    "                early_counter += 1\n",
    "                print(f\"EarlyStopping counter: {early_counter} out of {patience}\")\n",
    "            else:\n",
    "                break\n",
    "                print(\"Early stopping\")\n",
    "    \n",
    "    if not best_model_path.is_dir(): # there are no best model to save\n",
    "        return model\n",
    "    \n",
    "    best_model = BertForSequenceClassification.from_pretrained(best_model_path)\n",
    "    \n",
    "    ts = datetime.datetime.now()\n",
    "    ts_str = f\"model_{ts.day}{ts.month}{ts.year}{ts.hour}{ts.minute}{ts.second}\"\n",
    "    model.save_pretrained(folder / Path(ts_str))\n",
    "    \n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch_step = 2676 / 2676 / epoch = 1 | loss =  1.5528 acc =  0.3743: 100%|█| 2676/2676 [12:22<00:00, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1 / 100 | loss =  1.5528 acc =  0.3743 | val_loss =  1.3859 val_acc =  0.4557 val_prec =  0.3953 val_rec =  0.3188\n",
      "Validation loss decreased (inf --> 1.3859).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch_step = 2676 / 2676 / epoch = 2 | loss =  1.3461 acc =  0.4775: 100%|█| 2676/2676 [12:21<00:00, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 2 / 100 | loss =  1.3461 acc =  0.4775 | val_loss =  1.3111 val_acc =  0.4837 val_prec =  0.4073 val_rec =  0.3517\n",
      "Validation loss decreased (1.3859 --> 1.3111).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch_step = 2676 / 2676 / epoch = 3 | loss =  1.2764 acc =  0.5015: 100%|█| 2676/2676 [12:21<00:00, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 3 / 100 | loss =  1.2764 acc =  0.5015 | val_loss =  1.2915 val_acc =  0.4988 val_prec =  0.4272 val_rec =  0.3734\n",
      "Validation loss decreased (1.3111 --> 1.2915).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch_step = 2676 / 2676 / epoch = 4 | loss =  1.2291 acc =  0.5227: 100%|█| 2676/2676 [12:21<00:00, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 4 / 100 | loss =  1.2291 acc =  0.5227 | val_loss =  1.2693 val_acc =  0.5022 val_prec =  0.4247 val_rec =  0.4000\n",
      "Validation loss decreased (1.2915 --> 1.2693).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch_step = 2676 / 2676 / epoch = 5 | loss =  1.1864 acc =  0.5400: 100%|█| 2676/2676 [12:20<00:00, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 5 / 100 | loss =  1.1864 acc =  0.5400 | val_loss =  1.2633 val_acc =  0.5101 val_prec =  0.4186 val_rec =  0.4071\n",
      "Validation loss decreased (1.2693 --> 1.2633).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch_step = 2676 / 2676 / epoch = 6 | loss =  1.1502 acc =  0.5551: 100%|█| 2676/2676 [12:21<00:00, \n",
      "batch_step = 1 / 2676 / epoch = 7 | loss =  0.9280 acc =  0.6250:   0%|     | 0/2676 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 6 / 100 | loss =  1.1502 acc =  0.5551 | val_loss =  1.2795 val_acc =  0.5099 val_prec =  0.4311 val_rec =  0.4058\n",
      "EarlyStopping counter: 1 out of 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch_step = 2676 / 2676 / epoch = 7 | loss =  1.1108 acc =  0.5704: 100%|█| 2676/2676 [12:21<00:00, \n",
      "Bar desc:   0%|                                                             | 0/2676 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 7 / 100 | loss =  1.1108 acc =  0.5704 | val_loss =  1.2937 val_acc =  0.5037 val_prec =  0.4310 val_rec =  0.4080\n",
      "EarlyStopping counter: 2 out of 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch_step = 2676 / 2676 / epoch = 8 | loss =  1.0736 acc =  0.5891: 100%|█| 2676/2676 [12:19<00:00, \n",
      "Bar desc:   0%|                                                             | 0/2676 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 8 / 100 | loss =  1.0736 acc =  0.5891 | val_loss =  1.2999 val_acc =  0.4977 val_prec =  0.4372 val_rec =  0.4142\n",
      "EarlyStopping counter: 3 out of 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch_step = 2676 / 2676 / epoch = 9 | loss =  1.0407 acc =  0.6015: 100%|█| 2676/2676 [12:19<00:00, \n",
      "batch_step = 1 / 2676 / epoch = 10 | loss =  1.4688 acc =  0.3750:   0%|    | 0/2676 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 9 / 100 | loss =  1.0407 acc =  0.6015 | val_loss =  1.3017 val_acc =  0.5052 val_prec =  0.4236 val_rec =  0.4104\n",
      "EarlyStopping counter: 4 out of 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch_step = 2676 / 2676 / epoch = 10 | loss =  1.0054 acc =  0.6151: 100%|█| 2676/2676 [12:19<00:00,\n",
      "Bar desc:   0%|                                                             | 0/2676 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 10 / 100 | loss =  1.0054 acc =  0.6151 | val_loss =  1.3195 val_acc =  0.5002 val_prec =  0.4330 val_rec =  0.4166\n",
      "EarlyStopping counter: 5 out of 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch_step = 2676 / 2676 / epoch = 11 | loss =  0.9685 acc =  0.6331: 100%|█| 2676/2676 [12:19<00:00,\n",
      "Bar desc:   0%|                                                             | 0/2676 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 11 / 100 | loss =  0.9685 acc =  0.6331 | val_loss =  1.3482 val_acc =  0.4968 val_prec =  0.4195 val_rec =  0.4069\n",
      "EarlyStopping counter: 6 out of 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch_step = 2676 / 2676 / epoch = 12 | loss =  0.9292 acc =  0.6500: 100%|█| 2676/2676 [12:19<00:00,\n",
      "Bar desc:   0%|                                                             | 0/2676 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 12 / 100 | loss =  0.9292 acc =  0.6500 | val_loss =  1.3643 val_acc =  0.5086 val_prec =  0.4382 val_rec =  0.4173\n",
      "EarlyStopping counter: 7 out of 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch_step = 2676 / 2676 / epoch = 13 | loss =  0.8961 acc =  0.6639: 100%|█| 2676/2676 [12:19<00:00,\n",
      "batch_step = 1 / 2676 / epoch = 14 | loss =  0.7907 acc =  0.8750:   0%|    | 0/2676 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 13 / 100 | loss =  0.8961 acc =  0.6639 | val_loss =  1.3716 val_acc =  0.4977 val_prec =  0.4334 val_rec =  0.4107\n",
      "EarlyStopping counter: 8 out of 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch_step = 2676 / 2676 / epoch = 14 | loss =  0.8557 acc =  0.6845: 100%|█| 2676/2676 [12:19<00:00,\n",
      "Bar desc:   0%|                                                             | 0/2676 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 14 / 100 | loss =  0.8557 acc =  0.6845 | val_loss =  1.4314 val_acc =  0.4883 val_prec =  0.4134 val_rec =  0.4133\n",
      "EarlyStopping counter: 9 out of 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch_step = 2676 / 2676 / epoch = 15 | loss =  0.8229 acc =  0.6961: 100%|█| 2676/2676 [12:19<00:00,\n",
      "Bar desc:   0%|                                                             | 0/2676 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 15 / 100 | loss =  0.8229 acc =  0.6961 | val_loss =  1.4588 val_acc =  0.4874 val_prec =  0.4101 val_rec =  0.4098\n",
      "EarlyStopping counter: 10 out of 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch_step = 2676 / 2676 / epoch = 16 | loss =  0.7826 acc =  0.7112: 100%|█| 2676/2676 [12:19<00:00,\n",
      "Bar desc:   0%|                                                             | 0/2676 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 16 / 100 | loss =  0.7826 acc =  0.7112 | val_loss =  1.4856 val_acc =  0.4973 val_prec =  0.4195 val_rec =  0.4103\n",
      "EarlyStopping counter: 11 out of 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch_step = 2676 / 2676 / epoch = 17 | loss =  0.7484 acc =  0.7281: 100%|█| 2676/2676 [12:19<00:00,\n",
      "Bar desc:   0%|                                                             | 0/2676 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 17 / 100 | loss =  0.7484 acc =  0.7281 | val_loss =  1.5248 val_acc =  0.4935 val_prec =  0.4168 val_rec =  0.4203\n",
      "EarlyStopping counter: 12 out of 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch_step = 2676 / 2676 / epoch = 18 | loss =  0.7117 acc =  0.7423: 100%|█| 2676/2676 [12:19<00:00,\n",
      "Bar desc:   0%|                                                             | 0/2676 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 18 / 100 | loss =  0.7117 acc =  0.7423 | val_loss =  1.5615 val_acc =  0.4867 val_prec =  0.4138 val_rec =  0.4054\n",
      "EarlyStopping counter: 13 out of 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch_step = 2676 / 2676 / epoch = 19 | loss =  0.6713 acc =  0.7589: 100%|█| 2676/2676 [12:19<00:00,\n",
      "batch_step = 1 / 2676 / epoch = 20 | loss =  1.1724 acc =  0.6250:   0%|    | 0/2676 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 19 / 100 | loss =  0.6713 acc =  0.7589 | val_loss =  1.5870 val_acc =  0.4938 val_prec =  0.4142 val_rec =  0.4169\n",
      "EarlyStopping counter: 14 out of 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch_step = 2676 / 2676 / epoch = 20 | loss =  0.6390 acc =  0.7721: 100%|█| 2676/2676 [12:19<00:00,\n",
      "Bar desc:   0%|                                                             | 0/2676 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 20 / 100 | loss =  0.6390 acc =  0.7721 | val_loss =  1.6645 val_acc =  0.4938 val_prec =  0.4188 val_rec =  0.4110\n",
      "EarlyStopping counter: 15 out of 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch_step = 2676 / 2676 / epoch = 21 | loss =  0.6079 acc =  0.7822: 100%|█| 2676/2676 [12:19<00:00,\n",
      "Bar desc:   0%|                                                             | 0/2676 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 21 / 100 | loss =  0.6079 acc =  0.7822 | val_loss =  1.6988 val_acc =  0.4810 val_prec =  0.4125 val_rec =  0.4031\n",
      "EarlyStopping counter: 16 out of 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch_step = 2676 / 2676 / epoch = 22 | loss =  0.5748 acc =  0.7963: 100%|█| 2676/2676 [12:19<00:00,\n",
      "Bar desc:   0%|                                                             | 0/2676 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 22 / 100 | loss =  0.5748 acc =  0.7963 | val_loss =  1.7411 val_acc =  0.4948 val_prec =  0.4209 val_rec =  0.4299\n",
      "EarlyStopping counter: 17 out of 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch_step = 2676 / 2676 / epoch = 23 | loss =  0.5360 acc =  0.8155: 100%|█| 2676/2676 [12:19<00:00,\n",
      "Bar desc:   0%|                                                             | 0/2676 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 23 / 100 | loss =  0.5360 acc =  0.8155 | val_loss =  1.8132 val_acc =  0.4928 val_prec =  0.4203 val_rec =  0.4101\n",
      "EarlyStopping counter: 18 out of 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch_step = 2676 / 2676 / epoch = 24 | loss =  0.5116 acc =  0.8175: 100%|█| 2676/2676 [12:19<00:00,\n",
      "Bar desc:   0%|                                                             | 0/2676 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 24 / 100 | loss =  0.5116 acc =  0.8175 | val_loss =  1.8234 val_acc =  0.4872 val_prec =  0.4063 val_rec =  0.4121\n",
      "EarlyStopping counter: 19 out of 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch_step = 2676 / 2676 / epoch = 25 | loss =  0.4750 acc =  0.8348: 100%|█| 2676/2676 [12:19<00:00,\n",
      "Bar desc:   0%|                                                             | 0/2676 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 25 / 100 | loss =  0.4750 acc =  0.8348 | val_loss =  1.9152 val_acc =  0.4971 val_prec =  0.4065 val_rec =  0.4026\n",
      "EarlyStopping counter: 20 out of 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch_step = 2676 / 2676 / epoch = 26 | loss =  0.4501 acc =  0.8440: 100%|█| 2676/2676 [12:19<00:00,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 26 / 100 | loss =  0.4501 acc =  0.8440 | val_loss =  2.0114 val_acc =  0.4842 val_prec =  0.4126 val_rec =  0.4088\n"
     ]
    }
   ],
   "source": [
    "DIRECTORY = Path(f\"model_beto_classification/train_{datetime.datetime.timestamp(datetime.datetime.now()):0.0f}\")\n",
    "best_model = train(DIRECTORY, MODEL, CRITERION, OPTIMIZER, ITERATIONS, PATIENCE, TRAIN_LOADER, VAL_LOADER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anotamos los parámetros utilizados en el entrenamiento junto al modelo almacenado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(DIRECTORY / Path(\"parameters.txt\"), \"w\")\n",
    "f.write(f\"{lr = }\\n\" + \\\n",
    "        f\"{NUMBER_CLASSES = }\\n\" + \\\n",
    "        f\"{batchsize = }\"\n",
    "       )\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = best_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'val_loss = 1.2670 val_acc = 0.5081 val_prec = 0.4342 val_rec = 0.4197'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_loss, val_acc, val_prec, val_rec = evaluate(best_model.to(device), CRITERION, VAL_LOADER)\n",
    "f\"{val_loss = :0.4f} {val_acc = :0.4f} {val_prec = :0.4f} {val_rec = :0.4f}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test_loss = 1.2703 test_acc = 0.4955 test_prec = 0.4193 test_rec = 0.4026'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loss, test_acc, test_prec, test_rec = evaluate(best_model.to(device), CRITERION, TEST_LOADER)\n",
    "f\"{test_loss = :0.4f} {test_acc = :0.4f} {test_prec = :0.4f} {test_rec = :0.4f}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height: 4px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Con 3 clases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se ha entrenado el *dataset* agrupado y **balanceado en 3 categorías** (positivo con P y P+, neutro con NEU y NONE, y negativo con N y N+).\n",
    "Los siguientes resultados se han obtenido recorriendo el *dataset* en ***batches* de tamaño 8**, y con **una *patience* a 5 del *early stopping***. \n",
    "\n",
    "El tamaño de *batches* es el máximo que permite la memoria de la GPU donde se entrenó la red. Se ha elegido un valor bajo de *patience* para reducir el tiempo de entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| lr | Loss | Accuracy |\n",
    "|----|------|----------|\n",
    "|**1e-4**|1.1212|  0.3361  |\n",
    "|**1e-6**|0.7817|  0.6543  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| lr | Loss | Accuracy | Precision | Recall |\n",
    "|----|------|----------|-----------|--------|\n",
    "|**1e-4**|1.0990|  0.3336  |   0.3210  | 0.1143 |\n",
    "|**1e-6**|0.7997|  0.6608  |   0.6551  | 0.6411 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| lr | Loss | Accuracy | Precision | Recall |\n",
    "|----|------|----------|-----------|--------|\n",
    "|**1e-4**|1.0991|  0.3333  |   0.3383  | 0.1184 |\n",
    "|**1e-6**|0.7985|  0.6472  |   0.6308  | 0.6226 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Con 6 clases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se ha entrenado el *dataset* agrupado y **con las 6 categorías**.\n",
    "Los siguientes resultados se han obtenido recorriendo el *dataset* en ***batches* de tamaño 8**, y con **una *patience* a 5 del *early stopping***. \n",
    "\n",
    "El tamaño de *batches* es el máximo que permite la memoria de la GPU donde se entrenó la red. Se ha elegido un valor bajo de *patience* para reducir el tiempo de entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| lr | Loss | Accuracy |\n",
    "|----|------|----------|\n",
    "|**1e-4**|1.6653|  0.2551  |\n",
    "|**1e-6**|1.1864|  0.5400  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| lr | Loss | Accuracy | Precision | Recall |\n",
    "|----|------|----------|-----------|--------|\n",
    "|**1e-4**|1.6619|  0.2624  |  0.2228   | 0.0651 |\n",
    "|**1e-6**|1.2633|  0.5101  |  0.4186   | 0.4071 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| lr | Loss | Accuracy | Precision | Recall |\n",
    "|----|------|----------|-----------|--------|\n",
    "|**1e-4**|1.6625|  0.2609  |  0.2257   | 0.0703 |\n",
    "|**1e-6**|1.2703|  0.4955  |  0.4193   | 0.4026 |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
