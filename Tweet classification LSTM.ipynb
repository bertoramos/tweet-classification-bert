{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clasificación de tweets: LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autor: Alberto Ramos Sánchez"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contenido\n",
    "\n",
    "* [Dataset: TASS](#Dataset:-TASS)\n",
    "* [Preparar tweets](#Preparar-tweets)\n",
    "* [Crear dataset de entrenamiento](#Crear-dataset-de-entrenamiento)\n",
    "    * [Eliminar frases vacias](#Eliminar-frases-vacias)\n",
    "    * [Sets de entrenamiento, validación y test](#Sets-de-entrenamiento,-validación-y-test)\n",
    "    * [Tokenizamos texto](#Tokenizamos-texto)\n",
    "* [Modelo de clasificación: *LSTM*](#Modelo-de-clasificación:-*LSTM*)\n",
    "* [Entrenamiento](#Entrenamiento)\n",
    "    * [Evaluación](#Evaluación)\n",
    "* [Resultados](#Resultados)\n",
    "    * [Con 3 clases](#Con-3-clases)\n",
    "        * [Entrenamiento](#Entrenamiento)\n",
    "        * [Validación](#Validación)\n",
    "        * [Test](#Test)\n",
    "    * [Con 6 clases](#Con-6-clases)\n",
    "        * [Entrenamiento](#Entrenamiento)\n",
    "        * [Validation](#Validation)\n",
    "        * [Test](#Test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import SnowballStemmer\n",
    "stemmer = SnowballStemmer('spanish')\n",
    "\n",
    "import re\n",
    "import string\n",
    "import tqdm\n",
    "\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import Dataset, IterableDataset, DataLoader\n",
    "\n",
    "from pytorchtools.pytorchtools import EarlyStopping\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)\n",
    "\n",
    "# seed\n",
    "import random\n",
    "\n",
    "seed = 42\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este *notebook* se ha entrenado un modelo *LSTM* para realizar *sentiment analysis* sobre el conjunto de tweets en español de TASS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset: TASS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se ha trabajado con los *datasets* de tweets de [*TASS SEPLN*](http://tass.sepln.org/tass_data/download.php) desde el año 2012 al 2019. \n",
    "Todos los ficheros *xml* se han unido en un único dataset contenido en los ficheros *tweets.csv.gz*, *topics.csv.gz* y *polarities.csv.gz*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = Path(\"./TASS/conversion_result/dataset31k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets = pd.read_csv(dataset_path / Path(\"tweets.csv.gz\"), compression='gzip', header=0, sep=';', quotechar='\"')\n",
    "df_topics = pd.read_csv(dataset_path / Path(\"topics.csv.gz\"), compression='gzip', header=0, sep=';', quotechar='\"')\n",
    "df_polarities = pd.read_csv(dataset_path / Path(\"polarities.csv.gz\"), compression='gzip', header=0, sep=';', quotechar='\"')\n",
    "\n",
    "df_topics = df_topics.rename(columns={'tweetid_fk': 'tweetid'})\n",
    "df_polarities = df_polarities.rename(columns={'tweetid_fk': 'tweetid'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En total, en el dataset hay aproximadamente 31 mil tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Número total de tweets = 31375'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"Número total de tweets = {len(df_tweets)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y están etiquetados en 6 categorías:\n",
    "\n",
    "* P+ : Positivo fuerte (strong negative)\n",
    "* P : Positivo\n",
    "* NONE : Sin sentimiento (no sentiment tag)\n",
    "* NEU : Neutro\n",
    "* N : Negativo\n",
    "* N+ : Negativo fuerte (strong negative)\n",
    "\n",
    "En la siguiente tabla se muestra la cantidad de tweets por categoría:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Número de tweets</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>value</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>N</th>\n",
       "      <td>6219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N+</th>\n",
       "      <td>976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NEU</th>\n",
       "      <td>2755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NONE</th>\n",
       "      <td>5597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P</th>\n",
       "      <td>5442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P+</th>\n",
       "      <td>2793</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Número de tweets\n",
       "value                  \n",
       "N                  6219\n",
       "N+                  976\n",
       "NEU                2755\n",
       "NONE               5597\n",
       "P                  5442\n",
       "P+                 2793"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_polarities[['value', 'tweetid']].drop_duplicates(subset='tweetid', keep=\"first\").groupby(\"value\").count().rename(columns={\"value\": \"Categoría\", \"tweetid\": \"Número de tweets\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparar tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este apartado preprocesamos el texto de los tweets para entrenar el modelo.\n",
    "\n",
    "En la función *clean_tweet* limpiamos el texto quedándonos solamente con los caractéres alfanuméricos. Eliminamos los nombres de usuario, url, vocales seguidas repetidas más de dos veces, tabulaciones, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweet(text):\n",
    "    res_txt = re.sub(\"@\\w+\", \"\", text) # drop username\n",
    "    res_txt = re.sub(\"https?://[A-Za-z0-9\\./]+\", \"\", res_txt) # drop url\n",
    "    \n",
    "    for p in string.punctuation:\n",
    "        res_txt = res_txt.replace(p, \" \")\n",
    "    #res_txt = \" \".join([c for c in res_txt if c not in string.punctuation])\n",
    "    \n",
    "    # eliminar palabras con más de 2 vocales seguidas \"largooooo -> largoo\"\n",
    "    res_txt = re.sub(\"([A-Za-z])\\\\1{2,}\", \"\\\\1\\\\1\", res_txt)\n",
    "    \n",
    "    # eliminar espacios y tabulaciones repetidas\n",
    "    res_txt = re.sub(\"[ \\t]+\", \" \", res_txt.strip())\n",
    "    \n",
    "    # mantener solamente caracteres alfanuméricos\n",
    "    res_txt = re.sub(r'[^a-zñÑA-Z0-9áéíóúÁÉÍÓÚ ]', '', res_txt)\n",
    "    return res_txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En *prepare_tweet* eliminamos las *stopwords*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_tweet(text):\n",
    "    stop_words = nltk.corpus.stopwords.words('spanish')\n",
    "    custom_stop_words = [\"d\", \"q\"]\n",
    "    \n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [w for w in tokens if w not in stop_words]\n",
    "    tokens = [w for w in tokens if w not in custom_stop_words]\n",
    "    tokens = [w.lower() for w in tokens]\n",
    "    \n",
    "    # stemming\n",
    "    #tokens = [stemmer.stem(w) for w in tokens]\n",
    "    \n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplicamos el preprocesado, obteniendo el siguiente resultado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original :\n",
      " ;-)) RT @doloresvela: El #iPhone 5 podría ser presentado en marzo http://t.co/2kjKTjfF\n",
      "Resultado :\n",
      " rt el iphone 5 podría ser presentado marzo\n"
     ]
    }
   ],
   "source": [
    "print(f\"Original :\\n {df_tweets['content'][8235]}\")\n",
    "\n",
    "content = df_tweets['content']\n",
    "\n",
    "content = content.apply(lambda tweet: prepare_tweet(clean_tweet(str(tweet))))\n",
    "\n",
    "df_tweets['content'] = content\n",
    "\n",
    "print(f\"Resultado :\\n {df_tweets['content'][8235]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crear dataset de entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este apartado se crea el dataset de entrenamiento.\n",
    "\n",
    "Unimos los dataframes en uno solo, que contiene el id del tweet, el contenido del tweet y la categoría."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweetid</th>\n",
       "      <th>content</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>137228516625367040</td>\n",
       "      <td>en españa cosas pueden deben van hacer infinit...</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>137228522019229697</td>\n",
       "      <td>en pso corre vuela todavía caliente cadáver po...</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>137228533029277696</td>\n",
       "      <td>nomeolvido aprobo ley aborto libre todas minis...</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>137228551198998528</td>\n",
       "      <td>ccoo exige nuevo gobierno reactive mercado int...</td>\n",
       "      <td>NEU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>137228569750405120</td>\n",
       "      <td>a inviable parecen fraudes fiscales cometen mi...</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23777</th>\n",
       "      <td>819452909318504448</td>\n",
       "      <td>ya contando días volver vernos todavía voy</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23778</th>\n",
       "      <td>819456529543798784</td>\n",
       "      <td>gracias comedy central mtv voy nueva temporada...</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23779</th>\n",
       "      <td>819456610829471744</td>\n",
       "      <td>quiero necesito verte yaa</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23780</th>\n",
       "      <td>819469945167720448</td>\n",
       "      <td>demas solo den npm install instalen paquetes g...</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23781</th>\n",
       "      <td>819494275297656832</td>\n",
       "      <td>por si sabes tirar basura si tirado pedacito c...</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23782 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  tweetid                                            content  \\\n",
       "0      137228516625367040  en españa cosas pueden deben van hacer infinit...   \n",
       "1      137228522019229697  en pso corre vuela todavía caliente cadáver po...   \n",
       "2      137228533029277696  nomeolvido aprobo ley aborto libre todas minis...   \n",
       "3      137228551198998528  ccoo exige nuevo gobierno reactive mercado int...   \n",
       "4      137228569750405120  a inviable parecen fraudes fiscales cometen mi...   \n",
       "...                   ...                                                ...   \n",
       "23777  819452909318504448         ya contando días volver vernos todavía voy   \n",
       "23778  819456529543798784  gracias comedy central mtv voy nueva temporada...   \n",
       "23779  819456610829471744                          quiero necesito verte yaa   \n",
       "23780  819469945167720448  demas solo den npm install instalen paquetes g...   \n",
       "23781  819494275297656832  por si sabes tirar basura si tirado pedacito c...   \n",
       "\n",
       "      value  \n",
       "0         P  \n",
       "1         N  \n",
       "2         N  \n",
       "3       NEU  \n",
       "4         P  \n",
       "...     ...  \n",
       "23777     N  \n",
       "23778     P  \n",
       "23779     P  \n",
       "23780  NONE  \n",
       "23781     N  \n",
       "\n",
       "[23782 rows x 3 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_tweets = df_tweets[['tweetid', 'content']]\n",
    "data_sentim = df_polarities[['tweetid', 'value']]\n",
    "\n",
    "data_tweets = data_tweets.merge(data_sentim, on=\"tweetid\").drop_duplicates(subset='tweetid', keep=\"first\").reset_index(drop=True)\n",
    "data_tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se cambia la etiqueta por un valor numérico que indica la clase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweetid</th>\n",
       "      <th>content</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>137228516625367040</td>\n",
       "      <td>en españa cosas pueden deben van hacer infinit...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>137228522019229697</td>\n",
       "      <td>en pso corre vuela todavía caliente cadáver po...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>137228533029277696</td>\n",
       "      <td>nomeolvido aprobo ley aborto libre todas minis...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>137228551198998528</td>\n",
       "      <td>ccoo exige nuevo gobierno reactive mercado int...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>137228569750405120</td>\n",
       "      <td>a inviable parecen fraudes fiscales cometen mi...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23777</th>\n",
       "      <td>819452909318504448</td>\n",
       "      <td>ya contando días volver vernos todavía voy</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23778</th>\n",
       "      <td>819456529543798784</td>\n",
       "      <td>gracias comedy central mtv voy nueva temporada...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23779</th>\n",
       "      <td>819456610829471744</td>\n",
       "      <td>quiero necesito verte yaa</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23780</th>\n",
       "      <td>819469945167720448</td>\n",
       "      <td>demas solo den npm install instalen paquetes g...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23781</th>\n",
       "      <td>819494275297656832</td>\n",
       "      <td>por si sabes tirar basura si tirado pedacito c...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23782 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  tweetid                                            content  \\\n",
       "0      137228516625367040  en españa cosas pueden deben van hacer infinit...   \n",
       "1      137228522019229697  en pso corre vuela todavía caliente cadáver po...   \n",
       "2      137228533029277696  nomeolvido aprobo ley aborto libre todas minis...   \n",
       "3      137228551198998528  ccoo exige nuevo gobierno reactive mercado int...   \n",
       "4      137228569750405120  a inviable parecen fraudes fiscales cometen mi...   \n",
       "...                   ...                                                ...   \n",
       "23777  819452909318504448         ya contando días volver vernos todavía voy   \n",
       "23778  819456529543798784  gracias comedy central mtv voy nueva temporada...   \n",
       "23779  819456610829471744                          quiero necesito verte yaa   \n",
       "23780  819469945167720448  demas solo den npm install instalen paquetes g...   \n",
       "23781  819494275297656832  por si sabes tirar basura si tirado pedacito c...   \n",
       "\n",
       "       value  \n",
       "0          4  \n",
       "1          0  \n",
       "2          0  \n",
       "3          2  \n",
       "4          4  \n",
       "...      ...  \n",
       "23777      0  \n",
       "23778      4  \n",
       "23779      4  \n",
       "23780      3  \n",
       "23781      0  \n",
       "\n",
       "[23782 rows x 3 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label2id = {\"N\": 0,\n",
    "            \"N+\": 1,\n",
    "            \"NEU\": 2,\n",
    "            \"NONE\": 3,\n",
    "            \"P\": 4,\n",
    "            \"P+\": 5}\n",
    "\n",
    "NUMBER_CLASSES = 6\n",
    "\n",
    "data_tweets[\"value\"].replace(label2id, inplace=True)\n",
    "data_tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eliminar frases vacias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hay ciertas frases vacías que debemos eliminar pues el tokenizador utilizado no las acepta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweetid</th>\n",
       "      <th>content</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>137228516625367040</td>\n",
       "      <td>en españa cosas pueden deben van hacer infinit...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>137228522019229697</td>\n",
       "      <td>en pso corre vuela todavía caliente cadáver po...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>137228533029277696</td>\n",
       "      <td>nomeolvido aprobo ley aborto libre todas minis...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>137228551198998528</td>\n",
       "      <td>ccoo exige nuevo gobierno reactive mercado int...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>137228569750405120</td>\n",
       "      <td>a inviable parecen fraudes fiscales cometen mi...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23777</th>\n",
       "      <td>819452909318504448</td>\n",
       "      <td>ya contando días volver vernos todavía voy</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23778</th>\n",
       "      <td>819456529543798784</td>\n",
       "      <td>gracias comedy central mtv voy nueva temporada...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23779</th>\n",
       "      <td>819456610829471744</td>\n",
       "      <td>quiero necesito verte yaa</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23780</th>\n",
       "      <td>819469945167720448</td>\n",
       "      <td>demas solo den npm install instalen paquetes g...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23781</th>\n",
       "      <td>819494275297656832</td>\n",
       "      <td>por si sabes tirar basura si tirado pedacito c...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23754 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  tweetid                                            content  \\\n",
       "0      137228516625367040  en españa cosas pueden deben van hacer infinit...   \n",
       "1      137228522019229697  en pso corre vuela todavía caliente cadáver po...   \n",
       "2      137228533029277696  nomeolvido aprobo ley aborto libre todas minis...   \n",
       "3      137228551198998528  ccoo exige nuevo gobierno reactive mercado int...   \n",
       "4      137228569750405120  a inviable parecen fraudes fiscales cometen mi...   \n",
       "...                   ...                                                ...   \n",
       "23777  819452909318504448         ya contando días volver vernos todavía voy   \n",
       "23778  819456529543798784  gracias comedy central mtv voy nueva temporada...   \n",
       "23779  819456610829471744                          quiero necesito verte yaa   \n",
       "23780  819469945167720448  demas solo den npm install instalen paquetes g...   \n",
       "23781  819494275297656832  por si sabes tirar basura si tirado pedacito c...   \n",
       "\n",
       "       value  \n",
       "0          4  \n",
       "1          0  \n",
       "2          0  \n",
       "3          2  \n",
       "4          4  \n",
       "...      ...  \n",
       "23777      0  \n",
       "23778      4  \n",
       "23779      4  \n",
       "23780      3  \n",
       "23781      0  \n",
       "\n",
       "[23754 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_tweets = data_tweets[data_tweets['content'].apply(lambda x: len(x)) > 0]\n",
    "data_tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con la variable *apply_balance* controlamos si queremos aplicar o no balanceo de datos.\n",
    "\n",
    "En el caso de utilizar 6 clases, no aplicamos balanceo de datos, pues perderíamos la mayoría de los tweets debido a que la clase N+ tiene muchos menos tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_balance = True\n",
    "\n",
    "if apply_balance:\n",
    "    g = data_tweets.groupby('value')\n",
    "    data_tweets = g.apply(lambda x: x.sample(g.size().min()).reset_index(drop=True))\n",
    "    data_tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sets de entrenamiento, validación y test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dividimos el *dataset* en entrenamiento, validación y test. Con la opción *stratify* aseguramos que se mantenga la misma proporción de clases para cada conjunto que el *dataset* original. Para asegurarlo, es posible que se pierdan ciertos tweets. Esta operación no balancea el dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 0.90\n",
    "val_size = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5270, 3), (293, 3), (293, 3))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train, data_val = train_test_split(data_tweets, test_size=1-train_size, stratify=data_tweets[['value']])\n",
    "\n",
    "df_val, df_test = train_test_split(data_val, test_size=1-(val_size/(1-train_size)), stratify=data_val[['value']])\n",
    "\n",
    "df_train.shape, df_val.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guardamos de forma temporal el *dataset* en archivos csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path(\"./temp\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "df_train[['content', 'value']].to_csv(\"./temp/tweet_train.tsv\", index=False, sep=\"\\t\")\n",
    "df_val[['content', 'value']].to_csv(\"./temp/tweet_val.tsv\", index=False, sep=\"\\t\")\n",
    "df_test[['content', 'value']].to_csv(\"./temp/tweet_test.tsv\", index=False, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizamos texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El tokenizador nos convierte las palabras a indices, que son utilizadas por el *embedding* para crear la representación *word2vec*. Además, con la opción *include_lengths* a *True* añade la longitud de cada una de las frases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import TabularDataset\n",
    "from torchtext.data import Field\n",
    "\n",
    "tokenize = lambda x: x.split()\n",
    "\n",
    "TEXT = Field(tokenize=tokenize, include_lengths=True, batch_first=True, lower=True)\n",
    "LABEL = Field(sequential=False, use_vocab=False, batch_first=True)\n",
    "\n",
    "\n",
    "datafields = [(\"content\", TEXT),\n",
    "              (\"value\", LABEL)]\n",
    "\n",
    "train, validation, test = TabularDataset.splits(\n",
    "        path=\"./temp/\",\n",
    "        train=\"tweet_train.tsv\",\n",
    "        validation=\"tweet_val.tsv\",\n",
    "        test=\"tweet_test.tsv\",\n",
    "        format='tsv',\n",
    "        skip_header=True,\n",
    "        #csv_reader_params={\"delimiter\":\";\"},\n",
    "        fields=datafields)\n",
    "\n",
    "TEXT.build_vocab(train, min_freq=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import Iterator, BucketIterator\n",
    "\n",
    "TRAIN_ITER, VAL_ITER, TEST_ITER = BucketIterator.splits(\n",
    "    (train, validation, test),\n",
    "    batch_sizes=(10, 10, 10),\n",
    "    device=device,\n",
    "    sort_key=lambda x: len(x.content),\n",
    "    repeat=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   6,  290,    0,    0,    0,  734, 3093,  583,    0,  979,    0,    1,\n",
      "            1,    1,    1,    1,    1],\n",
      "        [  15,    0,    0, 2932, 2584,  160,  474,   21,    0,    0,  123,    0,\n",
      "            0,  244,    1,    1,    1],\n",
      "        [1656,  154,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1],\n",
      "        [1036,  323,    0,    0,    0,    0,   29,  226,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1],\n",
      "        [ 904,    0,  263,   16,   34,   19,  431,    0,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1],\n",
      "        [   0,    0, 1635, 1705,    0,    0, 1635, 1892,    4,  931,    0,    0,\n",
      "         1513,  343,   96,   43,   47],\n",
      "        [   0,    5,    4,  198,  900,    0,    0,  504,   23,    0,    1,    1,\n",
      "            1,    1,    1,    1,    1],\n",
      "        [   0,   55,   83, 2682,  567, 3109, 1490,   36,   31, 1398,    1,    1,\n",
      "            1,    1,    1,    1,    1],\n",
      "        [   3,  154,  117,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1],\n",
      "        [ 101, 1793, 1077,    0,    0, 1319,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1]], device='cuda:0')\n",
      "tensor([11, 14,  2,  8,  8, 17, 10, 10,  3,  6], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "for ((content, content_len), target), _ in TRAIN_ITER:\n",
    "    print(content)\n",
    "    print(content_len)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo de clasificación: *LSTM*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso utilizamos el modelo *LSTM* para clasificar los tweets. Este modelo se compone de una capa de *word embedding*, una red *LSTM* y una red *fully connected* para generar las etiquetas de salida. La función *pack_padded_sequence* empaqueta la entrada de la LSTM para tener en cuenta distintos tamaños de frases.\n",
    "\n",
    "El modelo tiene tres métodos para serializar: *load_from*, *save_to* y *checkpoint*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifierLSTMModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embed_dim, drop_dim, n_out):\n",
    "        super(ClassifierLSTMModel, self).__init__()\n",
    "        \n",
    "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size=embed_dim,\n",
    "                            hidden_size=embed_dim,\n",
    "                            num_layers=1,\n",
    "                            batch_first=True)\n",
    "        \n",
    "        self.classifier = nn.Linear(embed_dim, n_out)\n",
    "        \n",
    "        self.drop = nn.Dropout(drop_dim)\n",
    "    \n",
    "    \n",
    "    def forward(self, x, max_len):\n",
    "        \n",
    "        embeds_out = self.embed(x)\n",
    "        \n",
    "        packed_in = nn.utils.rnn.pack_padded_sequence(embeds_out, max_len, batch_first=True, enforce_sorted=False)\n",
    "        lstm_out, (lstm_hidden, lstm_cell) = self.lstm(packed_in)\n",
    "        \n",
    "        tags = self.classifier(self.drop(lstm_hidden[0]))\n",
    "        tag_score = F.log_softmax(self.drop(tags), dim=1)\n",
    "        \n",
    "        return tag_score\n",
    "    \n",
    "    def save_to(self, path):\n",
    "        path.mkdir(parents=True, exist_ok=True)\n",
    "        torch.save(self.state_dict(), path / Path(\"lstm_model.pt\"))\n",
    "    \n",
    "    @classmethod\n",
    "    def load_from(cls, folder, dev, vocab_size, embed_dim, drop_dim, n_out ):\n",
    "        model = cls(vocab_size, embed_dim, drop_dim, n_out)\n",
    "        state_dict = torch.load(folder / Path(\"lstm_model.pt\"))\n",
    "        model.load_state_dict(state_dict)\n",
    "        return model.to(dev)\n",
    "    \n",
    "    def checkpoint(self, folder, optimizer, stats):\n",
    "        status = {\n",
    "            'state_dict': self.state_dict(),\n",
    "            'optimizer': optimizer.state_dict()\n",
    "        }\n",
    "        status = {**status, **stats}\n",
    "        \n",
    "        ts = datetime.datetime.now()\n",
    "        ts_str = f\"checkpoint_{ts.day}{ts.month}{ts.year}{ts.hour}{ts.minute}{ts.second}\"\n",
    "        (folder / Path(ts_str)).mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        torch.save(status, folder / Path(ts_str) / Path(\"checkpoint.pth\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las dos siguientes funciones definen el entrenamiento y validación de la red. Para evitar sobrecargar la memoria de la GPU, las variables de cargan a CUDA por *batches* únicamente en el momento de utilizarlas.\n",
    "\n",
    "El modelo final se almacena en el directorio elegido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_WORDS = len(TEXT.vocab)\n",
    "EMBED_DIM = 32\n",
    "DROPSIZE = 0.2\n",
    "PATIENCE = 10\n",
    "\n",
    "ITERATIONS = 100\n",
    "\n",
    "\n",
    "MODEL = ClassifierLSTMModel(MAX_WORDS, EMBED_DIM, DROPSIZE, NUMBER_CLASSES).to(device)\n",
    "\n",
    "CRITERION = nn.CrossEntropyLoss()\n",
    "lr = 1e-4\n",
    "OPTIMIZER = torch.optim.Adam(MODEL.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate(model, loss_func, val_iter):\n",
    "    total_loss = 0\n",
    "    total_acc = 0\n",
    "    total_prec = 0\n",
    "    total_rec = 0\n",
    "    \n",
    "    for batch_step, ( ((content, content_len), target), _ ) in enumerate(val_iter, 1):\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = model(content, content_len)\n",
    "            \n",
    "            loss = loss_func(output, target)\n",
    "            \n",
    "            prediction = output.detach().cpu().argmax(dim=1)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            total_acc += accuracy_score(prediction, target.cpu())\n",
    "            \n",
    "            total_prec += precision_score(prediction, target.cpu(), average=\"macro\", zero_division=0.0)\n",
    "            total_rec += recall_score(prediction, target.cpu(), average=\"macro\", zero_division=0.0)\n",
    "    \n",
    "    T = len(val_iter)\n",
    "    return map(lambda r: r/T, [total_loss, total_acc, total_prec, total_rec])\n",
    "\n",
    "def train(folder, model, loss_func, optimizer, nepochs, train_iter, val_iter, patience):\n",
    "    model.train()\n",
    "    \n",
    "    ts = datetime.datetime.now()\n",
    "    best_model_path = Path(f\"./temp/best_model_lstm_{ts.day}{ts.month}{ts.year}{ts.hour}{ts.minute}{ts.second}\")\n",
    "    early_stopping = EarlyStopping(patience=patience, verbose=True, path=best_model_path)\n",
    "    \n",
    "    for epoch in range(1, nepochs+1):\n",
    "        total_loss = 0\n",
    "        total_acc = 0\n",
    "        total_prec = 0\n",
    "        total_rec = 0\n",
    "        \n",
    "        progress_bar = tqdm.tqdm(train_iter, desc=\"\")\n",
    "        \n",
    "        for batch_step, ( ((content, content_len), target), _ ) in enumerate(progress_bar, 1):\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            out = model(content, content_len)\n",
    "\n",
    "            loss = loss_func(out, target)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            total_acc += accuracy_score(out.detach().cpu().argmax(dim=1), target.detach().cpu())\n",
    "            \n",
    "            progress_bar.set_description(f\"{batch_step = } / {len(train_iter)} / {epoch = } | loss = {total_loss/batch_step : 0.4f} acc = {total_acc/batch_step : 0.4f}\")\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "        progress_bar.close()\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss, val_acc, val_prec, val_rec = evaluate(model, loss_func, val_iter)\n",
    "        model.train()\n",
    "        \n",
    "        print(\"*\"*10)\n",
    "        print(f\"Epoch {epoch} / {nepochs} | loss = {total_loss/len(train_iter) : 0.4f} acc = {total_acc/len(train_iter) : 0.4f} | {val_loss = : 0.4f} {val_acc = : 0.4f} {val_prec = :0.4f} {val_rec = :0.4f}\")\n",
    "        print(\"*\"*10)\n",
    "        \n",
    "        stats = {\n",
    "            'epoch': epoch,\n",
    "            'total_epochs': nepochs,\n",
    "            'loss': total_loss / len(train_iter),\n",
    "            'val_loss': val_loss,\n",
    "            'val_accuracy': val_acc,\n",
    "            'val_prec': val_prec,\n",
    "            'val_rec': val_rec\n",
    "        }\n",
    "        \n",
    "        model.checkpoint(folder=folder,\n",
    "                         optimizer=optimizer,\n",
    "                         stats=stats)\n",
    "        \n",
    "        early_stopping(val_loss, model)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "    if not best_model_path.is_dir(): # there are no best model to save\n",
    "        return\n",
    "    \n",
    "    best_model = ClassifierLSTMModel.load_from(folder=best_model_path,\n",
    "                                               dev=device,\n",
    "                                               vocab_size=MAX_WORDS,\n",
    "                                               embed_dim=EMBED_DIM,\n",
    "                                               drop_dim=DROPSIZE,\n",
    "                                               n_out=NUMBER_CLASSES)\n",
    "    \n",
    "    # save model\n",
    "    ts = datetime.datetime.now()\n",
    "    ts_str = f\"model_{ts.day}{ts.month}{ts.year}{ts.hour}{ts.minute}{ts.second}\"\n",
    "    best_model.save_to(folder / Path(ts_str))\n",
    "    return best_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch_step = 527 / 527 / epoch = 1 | loss =  1.8018 acc =  0.1717: 100%|████████████| 527/527 [00:03<00:00, 171.25it/s]\n",
      "batch_step = 32 / 527 / epoch = 2 | loss =  1.7910 acc =  0.1969:   3%|▍             | 18/527 [00:00<00:02, 173.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "Epoch 1 / 100 | loss =  1.8018 acc =  0.1717 | val_loss =  1.7823 val_acc =  0.2233 val_prec = 0.2364 val_rec = 0.1173\n",
      "**********\n",
      "Validation loss decreased (inf --> 1.782267).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch_step = 527 / 527 / epoch = 2 | loss =  1.7944 acc =  0.1759: 100%|████████████| 527/527 [00:02<00:00, 181.31it/s]\n",
      "batch_step = 32 / 527 / epoch = 3 | loss =  1.7956 acc =  0.1656:   3%|▍             | 17/527 [00:00<00:03, 167.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "Epoch 2 / 100 | loss =  1.7944 acc =  0.1759 | val_loss =  1.7801 val_acc =  0.2133 val_prec = 0.2194 val_rec = 0.1280\n",
      "**********\n",
      "Validation loss decreased (1.782267 --> 1.780077).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch_step = 527 / 527 / epoch = 3 | loss =  1.7931 acc =  0.1776: 100%|████████████| 527/527 [00:02<00:00, 176.84it/s]\n",
      "batch_step = 32 / 527 / epoch = 4 | loss =  1.7853 acc =  0.1875:   3%|▍             | 18/527 [00:00<00:02, 173.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "Epoch 3 / 100 | loss =  1.7931 acc =  0.1776 | val_loss =  1.7776 val_acc =  0.2056 val_prec = 0.2017 val_rec = 0.1321\n",
      "**********\n",
      "Validation loss decreased (1.780077 --> 1.777598).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch_step = 527 / 527 / epoch = 4 | loss =  1.7872 acc =  0.1799: 100%|████████████| 527/527 [00:02<00:00, 185.92it/s]\n",
      "batch_step = 33 / 527 / epoch = 5 | loss =  1.7866 acc =  0.2121:   3%|▍             | 18/527 [00:00<00:02, 178.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "Epoch 4 / 100 | loss =  1.7872 acc =  0.1799 | val_loss =  1.7744 val_acc =  0.2367 val_prec = 0.2395 val_rec = 0.1747\n",
      "**********\n",
      "Validation loss decreased (1.777598 --> 1.774424).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch_step = 527 / 527 / epoch = 5 | loss =  1.7853 acc =  0.1913: 100%|████████████| 527/527 [00:02<00:00, 184.80it/s]\n",
      "batch_step = 32 / 527 / epoch = 6 | loss =  1.7805 acc =  0.2156:   3%|▍             | 18/527 [00:00<00:02, 171.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "Epoch 5 / 100 | loss =  1.7853 acc =  0.1913 | val_loss =  1.7719 val_acc =  0.2467 val_prec = 0.2409 val_rec = 0.2044\n",
      "**********\n",
      "Validation loss decreased (1.774424 --> 1.771896).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch_step = 527 / 527 / epoch = 6 | loss =  1.7814 acc =  0.2008: 100%|████████████| 527/527 [00:02<00:00, 176.53it/s]\n",
      "batch_step = 32 / 527 / epoch = 7 | loss =  1.7804 acc =  0.1937:   3%|▍             | 17/527 [00:00<00:03, 168.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "Epoch 6 / 100 | loss =  1.7814 acc =  0.2008 | val_loss =  1.7684 val_acc =  0.2600 val_prec = 0.2515 val_rec = 0.2269\n",
      "**********\n",
      "Validation loss decreased (1.771896 --> 1.768448).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch_step = 527 / 527 / epoch = 7 | loss =  1.7770 acc =  0.2095: 100%|████████████| 527/527 [00:02<00:00, 177.31it/s]\n",
      "batch_step = 30 / 527 / epoch = 8 | loss =  1.7755 acc =  0.2000:   3%|▍             | 18/527 [00:00<00:02, 175.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "Epoch 7 / 100 | loss =  1.7770 acc =  0.2095 | val_loss =  1.7644 val_acc =  0.2667 val_prec = 0.2509 val_rec = 0.2299\n",
      "**********\n",
      "Validation loss decreased (1.768448 --> 1.764374).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch_step = 527 / 527 / epoch = 8 | loss =  1.7744 acc =  0.2082: 100%|████████████| 527/527 [00:02<00:00, 176.53it/s]\n",
      "batch_step = 31 / 527 / epoch = 9 | loss =  1.7742 acc =  0.1839:   3%|▍             | 17/527 [00:00<00:03, 167.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "Epoch 8 / 100 | loss =  1.7744 acc =  0.2082 | val_loss =  1.7595 val_acc =  0.2633 val_prec = 0.2405 val_rec = 0.2277\n",
      "**********\n",
      "Validation loss decreased (1.764374 --> 1.759478).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch_step = 527 / 527 / epoch = 9 | loss =  1.7679 acc =  0.2209: 100%|████████████| 527/527 [00:02<00:00, 176.48it/s]\n",
      "batch_step = 29 / 527 / epoch = 10 | loss =  1.7502 acc =  0.2759:   3%|▍            | 18/527 [00:00<00:02, 170.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "Epoch 9 / 100 | loss =  1.7679 acc =  0.2209 | val_loss =  1.7534 val_acc =  0.2833 val_prec = 0.2619 val_rec = 0.2307\n",
      "**********\n",
      "Validation loss decreased (1.759478 --> 1.753412).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch_step = 527 / 527 / epoch = 10 | loss =  1.7626 acc =  0.2252: 100%|███████████| 527/527 [00:02<00:00, 175.88it/s]\n",
      "batch_step = 33 / 527 / epoch = 11 | loss =  1.7420 acc =  0.2848:   3%|▍            | 18/527 [00:00<00:02, 178.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "Epoch 10 / 100 | loss =  1.7626 acc =  0.2252 | val_loss =  1.7472 val_acc =  0.2744 val_prec = 0.2443 val_rec = 0.2306\n",
      "**********\n",
      "Validation loss decreased (1.753412 --> 1.747194).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch_step = 527 / 527 / epoch = 11 | loss =  1.7579 acc =  0.2423: 100%|███████████| 527/527 [00:02<00:00, 185.84it/s]\n",
      "batch_step = 31 / 527 / epoch = 12 | loss =  1.7437 acc =  0.2581:   3%|▍            | 17/527 [00:00<00:03, 168.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "Epoch 11 / 100 | loss =  1.7579 acc =  0.2423 | val_loss =  1.7378 val_acc =  0.3033 val_prec = 0.2793 val_rec = 0.2577\n",
      "**********\n",
      "Validation loss decreased (1.747194 --> 1.737842).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch_step = 527 / 527 / epoch = 12 | loss =  1.7478 acc =  0.2499: 100%|███████████| 527/527 [00:03<00:00, 175.16it/s]\n",
      "batch_step = 30 / 527 / epoch = 13 | loss =  1.7417 acc =  0.2200:   3%|▍            | 17/527 [00:00<00:03, 167.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "Epoch 12 / 100 | loss =  1.7478 acc =  0.2499 | val_loss =  1.7268 val_acc =  0.3167 val_prec = 0.2877 val_rec = 0.2708\n",
      "**********\n",
      "Validation loss decreased (1.737842 --> 1.726828).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch_step = 527 / 527 / epoch = 13 | loss =  1.7402 acc =  0.2632: 100%|███████████| 527/527 [00:02<00:00, 176.71it/s]\n",
      "batch_step = 31 / 527 / epoch = 14 | loss =  1.7282 acc =  0.2452:   3%|▍            | 17/527 [00:00<00:03, 168.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "Epoch 13 / 100 | loss =  1.7402 acc =  0.2632 | val_loss =  1.7149 val_acc =  0.3200 val_prec = 0.2892 val_rec = 0.2671\n",
      "**********\n",
      "Validation loss decreased (1.726828 --> 1.714911).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch_step = 527 / 527 / epoch = 14 | loss =  1.7294 acc =  0.2598: 100%|███████████| 527/527 [00:03<00:00, 175.51it/s]\n",
      "batch_step = 30 / 527 / epoch = 15 | loss =  1.7192 acc =  0.2667:   3%|▍            | 17/527 [00:00<00:03, 167.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "Epoch 14 / 100 | loss =  1.7294 acc =  0.2598 | val_loss =  1.6997 val_acc =  0.3100 val_prec = 0.2818 val_rec = 0.2528\n",
      "**********\n",
      "Validation loss decreased (1.714911 --> 1.699671).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch_step = 527 / 527 / epoch = 15 | loss =  1.7234 acc =  0.2638: 100%|███████████| 527/527 [00:03<00:00, 175.00it/s]\n",
      "batch_step = 32 / 527 / epoch = 16 | loss =  1.7081 acc =  0.3063:   3%|▍            | 17/527 [00:00<00:03, 169.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "Epoch 15 / 100 | loss =  1.7234 acc =  0.2638 | val_loss =  1.6909 val_acc =  0.3200 val_prec = 0.2881 val_rec = 0.2607\n",
      "**********\n",
      "Validation loss decreased (1.699671 --> 1.690891).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch_step = 527 / 527 / epoch = 16 | loss =  1.7120 acc =  0.2700: 100%|███████████| 527/527 [00:03<00:00, 175.30it/s]\n",
      "batch_step = 31 / 527 / epoch = 17 | loss =  1.7331 acc =  0.2484:   3%|▍            | 17/527 [00:00<00:03, 167.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "Epoch 16 / 100 | loss =  1.7120 acc =  0.2700 | val_loss =  1.6821 val_acc =  0.3133 val_prec = 0.2861 val_rec = 0.2510\n",
      "**********\n",
      "Validation loss decreased (1.690891 --> 1.682115).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch_step = 527 / 527 / epoch = 17 | loss =  1.6985 acc =  0.2937: 100%|███████████| 527/527 [00:03<00:00, 174.24it/s]\n",
      "batch_step = 31 / 527 / epoch = 18 | loss =  1.6523 acc =  0.2935:   3%|▍            | 17/527 [00:00<00:03, 167.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "Epoch 17 / 100 | loss =  1.6985 acc =  0.2937 | val_loss =  1.6718 val_acc =  0.3300 val_prec = 0.2964 val_rec = 0.2560\n",
      "**********\n",
      "Validation loss decreased (1.682115 --> 1.671751).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch_step = 527 / 527 / epoch = 18 | loss =  1.6947 acc =  0.2937: 100%|███████████| 527/527 [00:03<00:00, 172.10it/s]\n",
      "batch_step = 28 / 527 / epoch = 19 | loss =  1.7015 acc =  0.2750:   3%|▍            | 17/527 [00:00<00:03, 162.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "Epoch 18 / 100 | loss =  1.6947 acc =  0.2937 | val_loss =  1.6663 val_acc =  0.3233 val_prec = 0.2955 val_rec = 0.2611\n",
      "**********\n",
      "Validation loss decreased (1.671751 --> 1.666321).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch_step = 527 / 527 / epoch = 19 | loss =  1.6793 acc =  0.2979: 100%|███████████| 527/527 [00:03<00:00, 173.26it/s]\n",
      "batch_step = 31 / 527 / epoch = 20 | loss =  1.7063 acc =  0.2516:   3%|▍            | 17/527 [00:00<00:03, 168.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "Epoch 19 / 100 | loss =  1.6793 acc =  0.2979 | val_loss =  1.6615 val_acc =  0.3300 val_prec = 0.2960 val_rec = 0.2749\n",
      "**********\n",
      "Validation loss decreased (1.666321 --> 1.661500).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch_step = 527 / 527 / epoch = 20 | loss =  1.6701 acc =  0.3163: 100%|███████████| 527/527 [00:03<00:00, 175.11it/s]\n",
      "batch_step = 30 / 527 / epoch = 21 | loss =  1.6562 acc =  0.3167:   3%|▍            | 17/527 [00:00<00:03, 163.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "Epoch 20 / 100 | loss =  1.6701 acc =  0.3163 | val_loss =  1.6544 val_acc =  0.3233 val_prec = 0.2941 val_rec = 0.2659\n",
      "**********\n",
      "Validation loss decreased (1.661500 --> 1.654406).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch_step = 527 / 527 / epoch = 21 | loss =  1.6648 acc =  0.3074: 100%|███████████| 527/527 [00:03<00:00, 166.83it/s]\n",
      "batch_step = 31 / 527 / epoch = 22 | loss =  1.6533 acc =  0.2839:   3%|▍            | 17/527 [00:00<00:03, 164.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "Epoch 21 / 100 | loss =  1.6648 acc =  0.3074 | val_loss =  1.6502 val_acc =  0.3267 val_prec = 0.2906 val_rec = 0.2630\n",
      "**********\n",
      "Validation loss decreased (1.654406 --> 1.650195).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch_step = 527 / 527 / epoch = 22 | loss =  1.6584 acc =  0.3102: 100%|███████████| 527/527 [00:03<00:00, 173.42it/s]\n",
      "batch_step = 32 / 527 / epoch = 23 | loss =  1.6906 acc =  0.2937:   3%|▍            | 17/527 [00:00<00:03, 165.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "Epoch 22 / 100 | loss =  1.6584 acc =  0.3102 | val_loss =  1.6447 val_acc =  0.3167 val_prec = 0.2869 val_rec = 0.2529\n",
      "**********\n",
      "Validation loss decreased (1.650195 --> 1.644710).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch_step = 527 / 527 / epoch = 23 | loss =  1.6486 acc =  0.3224: 100%|███████████| 527/527 [00:03<00:00, 173.51it/s]\n",
      "batch_step = 31 / 527 / epoch = 24 | loss =  1.5736 acc =  0.3806:   3%|▍            | 17/527 [00:00<00:03, 168.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "Epoch 23 / 100 | loss =  1.6486 acc =  0.3224 | val_loss =  1.6405 val_acc =  0.3200 val_prec = 0.2944 val_rec = 0.2550\n",
      "**********\n",
      "Validation loss decreased (1.644710 --> 1.640528).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch_step = 527 / 527 / epoch = 24 | loss =  1.6433 acc =  0.3271: 100%|███████████| 527/527 [00:03<00:00, 175.06it/s]\n",
      "batch_step = 31 / 527 / epoch = 25 | loss =  1.6422 acc =  0.3194:   3%|▍            | 18/527 [00:00<00:02, 172.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "Epoch 24 / 100 | loss =  1.6433 acc =  0.3271 | val_loss =  1.6404 val_acc =  0.3133 val_prec = 0.2832 val_rec = 0.2518\n",
      "**********\n",
      "Validation loss decreased (1.640528 --> 1.640388).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch_step = 527 / 527 / epoch = 25 | loss =  1.6295 acc =  0.3304: 100%|███████████| 527/527 [00:03<00:00, 172.24it/s]\n",
      "batch_step = 29 / 527 / epoch = 26 | loss =  1.6401 acc =  0.3345:   3%|▍            | 17/527 [00:00<00:03, 164.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "Epoch 25 / 100 | loss =  1.6295 acc =  0.3304 | val_loss =  1.6378 val_acc =  0.3067 val_prec = 0.2728 val_rec = 0.2401\n",
      "**********\n",
      "Validation loss decreased (1.640388 --> 1.637767).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch_step = 527 / 527 / epoch = 26 | loss =  1.6205 acc =  0.3353: 100%|███████████| 527/527 [00:03<00:00, 173.54it/s]\n",
      "batch_step = 31 / 527 / epoch = 27 | loss =  1.6491 acc =  0.3065:   3%|▍            | 18/527 [00:00<00:02, 175.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "Epoch 26 / 100 | loss =  1.6205 acc =  0.3353 | val_loss =  1.6318 val_acc =  0.3133 val_prec = 0.2776 val_rec = 0.2468\n",
      "**********\n",
      "Validation loss decreased (1.637767 --> 1.631799).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch_step = 527 / 527 / epoch = 27 | loss =  1.6154 acc =  0.3362: 100%|███████████| 527/527 [00:03<00:00, 173.30it/s]\n",
      "batch_step = 30 / 527 / epoch = 28 | loss =  1.6515 acc =  0.3267:   3%|▍            | 17/527 [00:00<00:03, 169.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "Epoch 27 / 100 | loss =  1.6154 acc =  0.3362 | val_loss =  1.6335 val_acc =  0.3033 val_prec = 0.2717 val_rec = 0.2431\n",
      "**********\n",
      "EarlyStopping counter: 1 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch_step = 527 / 527 / epoch = 28 | loss =  1.6033 acc =  0.3448: 100%|███████████| 527/527 [00:03<00:00, 173.65it/s]\n",
      "batch_step = 30 / 527 / epoch = 29 | loss =  1.6088 acc =  0.3233:   3%|▍            | 17/527 [00:00<00:03, 165.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "Epoch 28 / 100 | loss =  1.6033 acc =  0.3448 | val_loss =  1.6262 val_acc =  0.3000 val_prec = 0.2641 val_rec = 0.2413\n",
      "**********\n",
      "Validation loss decreased (1.631799 --> 1.626224).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch_step = 527 / 527 / epoch = 29 | loss =  1.5976 acc =  0.3454: 100%|███████████| 527/527 [00:03<00:00, 174.11it/s]\n",
      "batch_step = 29 / 527 / epoch = 30 | loss =  1.5369 acc =  0.4241:   3%|▍            | 17/527 [00:00<00:03, 165.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "Epoch 29 / 100 | loss =  1.5976 acc =  0.3454 | val_loss =  1.6219 val_acc =  0.3200 val_prec = 0.2913 val_rec = 0.2507\n",
      "**********\n",
      "Validation loss decreased (1.626224 --> 1.621914).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch_step = 527 / 527 / epoch = 30 | loss =  1.5882 acc =  0.3552: 100%|███████████| 527/527 [00:03<00:00, 174.04it/s]\n",
      "batch_step = 32 / 527 / epoch = 31 | loss =  1.5564 acc =  0.3937:   3%|▍            | 17/527 [00:00<00:03, 168.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "Epoch 30 / 100 | loss =  1.5882 acc =  0.3552 | val_loss =  1.6235 val_acc =  0.3100 val_prec = 0.2801 val_rec = 0.2412\n",
      "**********\n",
      "EarlyStopping counter: 1 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch_step = 527 / 527 / epoch = 31 | loss =  1.5811 acc =  0.3594: 100%|███████████| 527/527 [00:03<00:00, 174.57it/s]\n",
      "batch_step = 31 / 527 / epoch = 32 | loss =  1.5528 acc =  0.3677:   3%|▍            | 17/527 [00:00<00:03, 168.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "Epoch 31 / 100 | loss =  1.5811 acc =  0.3594 | val_loss =  1.6196 val_acc =  0.3000 val_prec = 0.2716 val_rec = 0.2327\n",
      "**********\n",
      "Validation loss decreased (1.621914 --> 1.619595).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch_step = 527 / 527 / epoch = 32 | loss =  1.5709 acc =  0.3524: 100%|███████████| 527/527 [00:03<00:00, 174.82it/s]\n",
      "batch_step = 31 / 527 / epoch = 33 | loss =  1.5911 acc =  0.3452:   3%|▍            | 17/527 [00:00<00:03, 167.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "Epoch 32 / 100 | loss =  1.5709 acc =  0.3524 | val_loss =  1.6213 val_acc =  0.3033 val_prec = 0.2781 val_rec = 0.2422\n",
      "**********\n",
      "EarlyStopping counter: 1 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch_step = 527 / 527 / epoch = 33 | loss =  1.5594 acc =  0.3657: 100%|███████████| 527/527 [00:03<00:00, 173.73it/s]\n",
      "batch_step = 30 / 527 / epoch = 34 | loss =  1.5467 acc =  0.3600:   3%|▍            | 17/527 [00:00<00:03, 168.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "Epoch 33 / 100 | loss =  1.5594 acc =  0.3657 | val_loss =  1.6199 val_acc =  0.3133 val_prec = 0.2827 val_rec = 0.2404\n",
      "**********\n",
      "EarlyStopping counter: 2 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch_step = 527 / 527 / epoch = 34 | loss =  1.5465 acc =  0.3727: 100%|███████████| 527/527 [00:03<00:00, 174.13it/s]\n",
      "batch_step = 30 / 527 / epoch = 35 | loss =  1.5374 acc =  0.4100:   3%|▍            | 18/527 [00:00<00:02, 173.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "Epoch 34 / 100 | loss =  1.5465 acc =  0.3727 | val_loss =  1.6288 val_acc =  0.2967 val_prec = 0.2713 val_rec = 0.2405\n",
      "**********\n",
      "EarlyStopping counter: 3 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch_step = 527 / 527 / epoch = 35 | loss =  1.5422 acc =  0.3769: 100%|███████████| 527/527 [00:03<00:00, 175.21it/s]\n",
      "batch_step = 29 / 527 / epoch = 36 | loss =  1.5770 acc =  0.3690:   3%|▍            | 17/527 [00:00<00:03, 165.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "Epoch 35 / 100 | loss =  1.5422 acc =  0.3769 | val_loss =  1.6150 val_acc =  0.3167 val_prec = 0.2911 val_rec = 0.2435\n",
      "**********\n",
      "Validation loss decreased (1.619595 --> 1.614986).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch_step = 527 / 527 / epoch = 36 | loss =  1.5477 acc =  0.3693: 100%|███████████| 527/527 [00:03<00:00, 175.13it/s]\n",
      "batch_step = 30 / 527 / epoch = 37 | loss =  1.5520 acc =  0.3567:   3%|▍            | 17/527 [00:00<00:03, 168.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "Epoch 36 / 100 | loss =  1.5477 acc =  0.3693 | val_loss =  1.6194 val_acc =  0.3033 val_prec = 0.2807 val_rec = 0.2387\n",
      "**********\n",
      "EarlyStopping counter: 1 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch_step = 527 / 527 / epoch = 37 | loss =  1.5344 acc =  0.3810: 100%|███████████| 527/527 [00:03<00:00, 174.35it/s]\n",
      "batch_step = 30 / 527 / epoch = 38 | loss =  1.5536 acc =  0.3733:   3%|▍            | 17/527 [00:00<00:03, 168.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "Epoch 37 / 100 | loss =  1.5344 acc =  0.3810 | val_loss =  1.6165 val_acc =  0.2933 val_prec = 0.2717 val_rec = 0.2295\n",
      "**********\n",
      "EarlyStopping counter: 2 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch_step = 527 / 527 / epoch = 38 | loss =  1.5251 acc =  0.3943: 100%|███████████| 527/527 [00:03<00:00, 175.45it/s]\n",
      "batch_step = 31 / 527 / epoch = 39 | loss =  1.4767 acc =  0.4484:   3%|▍            | 17/527 [00:00<00:03, 162.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "Epoch 38 / 100 | loss =  1.5251 acc =  0.3943 | val_loss =  1.6175 val_acc =  0.2933 val_prec = 0.2731 val_rec = 0.2322\n",
      "**********\n",
      "EarlyStopping counter: 3 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch_step = 527 / 527 / epoch = 39 | loss =  1.5200 acc =  0.3896: 100%|███████████| 527/527 [00:03<00:00, 175.26it/s]\n",
      "batch_step = 31 / 527 / epoch = 40 | loss =  1.5511 acc =  0.3645:   3%|▍            | 18/527 [00:00<00:03, 168.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "Epoch 39 / 100 | loss =  1.5200 acc =  0.3896 | val_loss =  1.6211 val_acc =  0.3133 val_prec = 0.2907 val_rec = 0.2592\n",
      "**********\n",
      "EarlyStopping counter: 4 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch_step = 527 / 527 / epoch = 40 | loss =  1.5097 acc =  0.3992: 100%|███████████| 527/527 [00:03<00:00, 174.85it/s]\n",
      "batch_step = 29 / 527 / epoch = 41 | loss =  1.5648 acc =  0.3448:   3%|▍            | 17/527 [00:00<00:03, 165.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "Epoch 40 / 100 | loss =  1.5097 acc =  0.3992 | val_loss =  1.6148 val_acc =  0.3200 val_prec = 0.2933 val_rec = 0.2524\n",
      "**********\n",
      "Validation loss decreased (1.614986 --> 1.614820).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch_step = 527 / 527 / epoch = 41 | loss =  1.5048 acc =  0.3926: 100%|███████████| 527/527 [00:03<00:00, 174.42it/s]\n",
      "batch_step = 30 / 527 / epoch = 42 | loss =  1.4738 acc =  0.3933:   3%|▍            | 17/527 [00:00<00:03, 163.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "Epoch 41 / 100 | loss =  1.5048 acc =  0.3926 | val_loss =  1.6232 val_acc =  0.3000 val_prec = 0.2696 val_rec = 0.2315\n",
      "**********\n",
      "EarlyStopping counter: 1 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch_step = 527 / 527 / epoch = 42 | loss =  1.4947 acc =  0.3977: 100%|███████████| 527/527 [00:03<00:00, 174.08it/s]\n",
      "batch_step = 31 / 527 / epoch = 43 | loss =  1.5265 acc =  0.3968:   3%|▍            | 17/527 [00:00<00:03, 160.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "Epoch 42 / 100 | loss =  1.4947 acc =  0.3977 | val_loss =  1.6232 val_acc =  0.3033 val_prec = 0.2727 val_rec = 0.2335\n",
      "**********\n",
      "EarlyStopping counter: 2 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch_step = 527 / 527 / epoch = 43 | loss =  1.4876 acc =  0.3979: 100%|███████████| 527/527 [00:03<00:00, 174.12it/s]\n",
      "batch_step = 32 / 527 / epoch = 44 | loss =  1.4341 acc =  0.4719:   3%|▍            | 18/527 [00:00<00:02, 171.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "Epoch 43 / 100 | loss =  1.4876 acc =  0.3979 | val_loss =  1.6238 val_acc =  0.3067 val_prec = 0.2752 val_rec = 0.2376\n",
      "**********\n",
      "EarlyStopping counter: 3 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch_step = 527 / 527 / epoch = 44 | loss =  1.4755 acc =  0.4121: 100%|███████████| 527/527 [00:03<00:00, 174.89it/s]\n",
      "batch_step = 30 / 527 / epoch = 45 | loss =  1.4814 acc =  0.3967:   3%|▍            | 17/527 [00:00<00:03, 167.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "Epoch 44 / 100 | loss =  1.4755 acc =  0.4121 | val_loss =  1.6265 val_acc =  0.3200 val_prec = 0.2915 val_rec = 0.2542\n",
      "**********\n",
      "EarlyStopping counter: 4 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch_step = 527 / 527 / epoch = 45 | loss =  1.4642 acc =  0.4120: 100%|███████████| 527/527 [00:03<00:00, 173.41it/s]\n",
      "batch_step = 29 / 527 / epoch = 46 | loss =  1.4358 acc =  0.4345:   3%|▍            | 16/527 [00:00<00:03, 155.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "Epoch 45 / 100 | loss =  1.4642 acc =  0.4120 | val_loss =  1.6248 val_acc =  0.3267 val_prec = 0.2978 val_rec = 0.2663\n",
      "**********\n",
      "EarlyStopping counter: 5 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch_step = 527 / 527 / epoch = 46 | loss =  1.4613 acc =  0.4165: 100%|███████████| 527/527 [00:03<00:00, 173.37it/s]\n",
      "batch_step = 30 / 527 / epoch = 47 | loss =  1.4410 acc =  0.4500:   3%|▍            | 17/527 [00:00<00:03, 166.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "Epoch 46 / 100 | loss =  1.4613 acc =  0.4165 | val_loss =  1.6298 val_acc =  0.3133 val_prec = 0.2739 val_rec = 0.2418\n",
      "**********\n",
      "EarlyStopping counter: 6 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch_step = 527 / 527 / epoch = 47 | loss =  1.4653 acc =  0.4205: 100%|███████████| 527/527 [00:03<00:00, 174.90it/s]\n",
      "batch_step = 32 / 527 / epoch = 48 | loss =  1.3685 acc =  0.4563:   3%|▍            | 17/527 [00:00<00:03, 168.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "Epoch 47 / 100 | loss =  1.4653 acc =  0.4205 | val_loss =  1.6248 val_acc =  0.3200 val_prec = 0.2926 val_rec = 0.2650\n",
      "**********\n",
      "EarlyStopping counter: 7 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch_step = 527 / 527 / epoch = 48 | loss =  1.4483 acc =  0.4245: 100%|███████████| 527/527 [00:02<00:00, 175.74it/s]\n",
      "batch_step = 31 / 527 / epoch = 49 | loss =  1.4350 acc =  0.4194:   3%|▍            | 18/527 [00:00<00:02, 170.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "Epoch 48 / 100 | loss =  1.4483 acc =  0.4245 | val_loss =  1.6296 val_acc =  0.3267 val_prec = 0.2984 val_rec = 0.2670\n",
      "**********\n",
      "EarlyStopping counter: 8 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch_step = 527 / 527 / epoch = 49 | loss =  1.4378 acc =  0.4304: 100%|███████████| 527/527 [00:03<00:00, 174.96it/s]\n",
      "batch_step = 30 / 527 / epoch = 50 | loss =  1.4296 acc =  0.4400:   3%|▍            | 17/527 [00:00<00:03, 167.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "Epoch 49 / 100 | loss =  1.4378 acc =  0.4304 | val_loss =  1.6385 val_acc =  0.3300 val_prec = 0.3093 val_rec = 0.2781\n",
      "**********\n",
      "EarlyStopping counter: 9 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch_step = 527 / 527 / epoch = 50 | loss =  1.4270 acc =  0.4406: 100%|███████████| 527/527 [00:03<00:00, 175.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "Epoch 50 / 100 | loss =  1.4270 acc =  0.4406 | val_loss =  1.6307 val_acc =  0.3400 val_prec = 0.3049 val_rec = 0.2725\n",
      "**********\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n"
     ]
    }
   ],
   "source": [
    "DIRECTORY = Path(f\"modelos_LSTM/train_{datetime.datetime.timestamp(datetime.datetime.now()):0.0f}\")\n",
    "best_model = train(DIRECTORY, MODEL, CRITERION, OPTIMIZER, ITERATIONS, TRAIN_ITER, VAL_ITER, PATIENCE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(DIRECTORY / Path(\"parameters.txt\"), \"w\")\n",
    "f.write(f\"{DROPSIZE = }\\n\" + \\\n",
    "        f\"{EMBED_DIM = }\\n\" + \\\n",
    "        f\"{NUMBER_CLASSES = }\\n\" + \\\n",
    "        f\"{lr = }\\n\"\n",
    "       )\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'val_loss = 1.6977 val_acc = 0.2956 val_prec = 0.2330 val_rec = 0.2236'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_loss, val_acc, val_prec, val_rec = evaluate(best_model, CRITERION, VAL_ITER)\n",
    "f\"{val_loss = :0.4f} {val_acc = :0.4f} {val_prec = :0.4f} {val_rec = :0.4f}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'val_loss = 1.7128 val_acc = 0.2589 val_prec = 0.2046 val_rec = 0.2204'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_loss, val_acc, val_prec, val_rec = evaluate(best_model, CRITERION, TEST_ITER)\n",
    "f\"{val_loss = :0.4f} {val_acc = :0.4f} {val_prec = :0.4f} {val_rec = :0.4f}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height: 4px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "table {float:left}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Con 3 clases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se ha entrenado el *dataset* agrupado y **balanceado en 3 categorías** (positivo con P y P+, neutro con NEU y NONE, y negativo con N y N+).\n",
    "Los siguientes resultados se han obtenido recorriendo el *dataset* en ***batches* de tamaño 10**, y con **una *patience* a 10 del *early stopping***. \n",
    "\n",
    "El tamaño de *batches* es el máximo que permite la memoria de la GPU donde se entrenó la red."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "<thead>\n",
    "  <tr>\n",
    "    <th>Dropout</th>\n",
    "    <th>Embedding</th>\n",
    "    <th>8</th>\n",
    "    <th>16</th>\n",
    "    <th>32</th>\n",
    "  </tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "  <tr>\n",
    "    <td rowspan=\"2\"><b>0.1</b></td>\n",
    "    <td><b>Loss</b></td>\n",
    "    <td>0.8802</td>\n",
    "    <td>0.8702</td>\n",
    "    <td>0.9284</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><b>Accuracy</b></td>\n",
    "    <td>0.5995</td>\n",
    "    <td>0.6027</td>\n",
    "    <td>0.5596</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td rowspan=\"2\"><b>0.2</b></td>\n",
    "    <td><b>Loss</b></td>\n",
    "    <td>0.8665</td>\n",
    "    <td>0.8787</td>\n",
    "    <td>0.8703</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><b>Accuracy</b></td>\n",
    "    <td>0.6111</td>\n",
    "    <td>0.6004</td>\n",
    "    <td>0.6006</td>\n",
    "  </tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "<thead>\n",
    "  <tr>\n",
    "    <th>Dropout</th>\n",
    "    <th>Embedding</th>\n",
    "    <th>8</th>\n",
    "    <th>16</th>\n",
    "    <th>32</th>\n",
    "  </tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "  <tr>\n",
    "    <td rowspan=\"4\"><b>0.1</b></td>\n",
    "    <td><b>Loss</b></td>\n",
    "    <td>0.9884</td>\n",
    "    <td>0.9818</td>\n",
    "    <td>1.0013</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><b>Accuracy</b></td>\n",
    "    <td>0.5174</td>\n",
    "    <td>0.5302</td>\n",
    "    <td>0.5211</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><b>Precision</b></td>\n",
    "    <td>0.4855</td>\n",
    "    <td>0.5046</td>\n",
    "    <td>0.4964</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><b>Recall</b></td>\n",
    "    <td>0.5045</td>\n",
    "    <td>0.5110</td>\n",
    "    <td>0.5044</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td rowspan=\"4\"><b>0.2</b></td>\n",
    "    <td><b>Loss</b></td>\n",
    "    <td>0.9812</td>\n",
    "    <td>0.9684</td>\n",
    "    <td>0.9940</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><b>Accuracy</b></td>\n",
    "    <td>0.5174</td>\n",
    "    <td>0.5340</td>\n",
    "    <td>0.5384</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><b>Precision</b></td>\n",
    "    <td>0.4892</td>\n",
    "    <td>0.5193</td>\n",
    "    <td>0.5148</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><b>Recall</b></td>\n",
    "    <td>0.5102</td>\n",
    "    <td>0.5153</td>\n",
    "    <td>0.5243</td>\n",
    "  </tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "<thead>\n",
    "  <tr>\n",
    "    <th>Dropout</th>\n",
    "    <th>Embedding</th>\n",
    "    <th>8</th>\n",
    "    <th>16</th>\n",
    "    <th>32</th>\n",
    "  </tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "  <tr>\n",
    "    <td rowspan=\"4\"><b>0.1</b></td>\n",
    "    <td><b>Loss</b></td>\n",
    "    <td>1.0188</td>\n",
    "    <td>1.0103</td>\n",
    "    <td>1.0125</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><b>Accuracy</b></td>\n",
    "    <td>0.5130</td>\n",
    "    <td>0.4963</td>\n",
    "    <td>0.5009</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><b>Precision</b></td>\n",
    "    <td>0.4911</td>\n",
    "    <td>0.4607</td>\n",
    "    <td>0.4874</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><b>Recall</b></td>\n",
    "    <td>0.4979</td>\n",
    "    <td>0.4684</td>\n",
    "    <td>0.4853</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td rowspan=\"4\"><b>0.2</b></td>\n",
    "    <td><b>Loss</b></td>\n",
    "    <td>1.0250</td>\n",
    "    <td>1.0193</td>\n",
    "    <td>1.0210</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><b>Accuracy</b></td>\n",
    "    <td>0.5093</td>\n",
    "    <td>0.5037</td>\n",
    "    <td>0.4815</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><b>Precision</b></td>\n",
    "    <td>0.4941</td>\n",
    "    <td>0.4697</td>\n",
    "    <td>0.4637</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><b>Recall</b></td>\n",
    "    <td>0.5014</td>\n",
    "    <td>0.4758</td>\n",
    "    <td>0.4644</td>\n",
    "  </tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Con 6 clases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se ha entrenado el *dataset* agrupado y **con las 6 categorías**.\n",
    "Los siguientes resultados se han obtenido recorriendo el *dataset* en ***batches* de tamaño 10**, y con **una *patience* a 10 del *early stopping***. \n",
    "\n",
    "El tamaño de *batches* es el máximo que permite la memoria de la GPU donde se entrenó la red."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "<thead>\n",
    "  <tr>\n",
    "    <th>Dropout</th>\n",
    "    <th>Embedding</th>\n",
    "    <th>8</th>\n",
    "    <th>16</th>\n",
    "    <th>32</th>\n",
    "  </tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "  <tr>\n",
    "    <td rowspan=\"2\"><b>0.1</b></td>\n",
    "    <td><b>Loss</b></td>\n",
    "    <td>1.5451</td>\n",
    "    <td>1.4947</td>\n",
    "    <td>1.5269</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><b>Accuracy</b></td>\n",
    "    <td>0.3767</td>\n",
    "    <td>0.4080</td>\n",
    "    <td>0.3915</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td rowspan=\"2\"><b>0.2</b></td>\n",
    "    <td><b>Loss</b></td>\n",
    "    <td>1.6179</td>\n",
    "    <td>1.5680</td>\n",
    "    <td>1.5097</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><b>Accuracy</b></td>\n",
    "    <td>0.3353</td>\n",
    "    <td>0.3600</td>\n",
    "    <td>0.3992</td>\n",
    "  </tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "<thead>\n",
    "  <tr>\n",
    "    <th>Dropout</th>\n",
    "    <th>Embedding</th>\n",
    "    <th>8</th>\n",
    "    <th>16</th>\n",
    "    <th>32</th>\n",
    "  </tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "  <tr>\n",
    "    <td rowspan=\"4\"><b>0.1</b></td>\n",
    "    <td><b>Loss</b></td>\n",
    "    <td>1.7536</td>\n",
    "    <td>1.6718</td>\n",
    "    <td>1.6179</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><b>Accuracy</b></td>\n",
    "    <td>0.2833</td>\n",
    "    <td>0.3322</td>\n",
    "    <td>0.3067</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><b>Precision</b></td>\n",
    "    <td>0.2507</td>\n",
    "    <td>0.2780</td>\n",
    "    <td>0.2792</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><b>Recall</b></td>\n",
    "    <td>0.2338</td>\n",
    "    <td>0.2582</td>\n",
    "    <td>0.2367</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td rowspan=\"4\"><b>0.2</b></td>\n",
    "    <td><b>Loss</b></td>\n",
    "    <td>1.7469</td>\n",
    "    <td>1.6689</td>\n",
    "    <td>1.6148</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><b>Accuracy</b></td>\n",
    "    <td>0.2522</td>\n",
    "    <td>0.3389</td>\n",
    "    <td>0.3200</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><b>Precision</b></td>\n",
    "    <td>0.2075</td>\n",
    "    <td>0.2825</td>\n",
    "    <td>0.2933</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><b>Recall</b></td>\n",
    "    <td>0.1968</td>\n",
    "    <td>0.2664</td>\n",
    "    <td>0.2524</td>\n",
    "  </tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "<thead>\n",
    "  <tr>\n",
    "    <th>Dropout</th>\n",
    "    <th>Embedding</th>\n",
    "    <th>8</th>\n",
    "    <th>16</th>\n",
    "    <th>32</th>\n",
    "  </tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "  <tr>\n",
    "    <td rowspan=\"4\"><b>0.1</b></td>\n",
    "    <td><b>Loss</b></td>\n",
    "    <td>1.7554</td>\n",
    "    <td>1.7520</td>\n",
    "    <td>1.7185</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><b>Accuracy</b></td>\n",
    "    <td>0.2489</td>\n",
    "    <td>0.2967</td>\n",
    "    <td>0.2678</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><b>Precision</b></td>\n",
    "    <td>0.2007</td>\n",
    "    <td>0.2492</td>\n",
    "    <td>0.2152</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><b>Recall</b></td>\n",
    "    <td>0.2048</td>\n",
    "    <td>0.2537</td>\n",
    "    <td>0.2120</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td rowspan=\"4\"><b>0.2</b></td>\n",
    "    <td><b>Loss</b></td>\n",
    "    <td>1.7487</td>\n",
    "    <td>1.7579</td>\n",
    "    <td>1.7128</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><b>Accuracy</b></td>\n",
    "    <td>0.2489</td>\n",
    "    <td>0.2800</td>\n",
    "    <td>0.2589</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><b>Precision</b></td>\n",
    "    <td>0.2015</td>\n",
    "    <td>0.2348</td>\n",
    "    <td>0.2046</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><b>Recall</b></td>\n",
    "    <td>0.1851</td>\n",
    "    <td>0.2444</td>\n",
    "    <td>0.2204</td>\n",
    "  </tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
